{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNTDyOuAj1l1cnbYuCH/Atf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AngeMervaneJdev/ML_AI_1/blob/main/tf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==1.14"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjLlEMiz1BrC",
        "outputId": "9f10dd6d-b8e1-4ffe-f69f-a11cf9b99a04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==1.14 in /usr/local/lib/python3.7/dist-packages (1.14.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (0.37.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.0.8)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.0.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.14.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (3.17.3)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.44.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.14.0)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.14.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.21.5)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (0.2.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (0.5.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.15.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (0.8.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14) (3.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.3.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (4.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.7.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.6->tensorflow==1.14) (1.5.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 1.x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHfoAhbR4EZd",
        "outputId": "389867ca-69e3-43c5-bdd3-14f852203cb3"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "ZDsqVacwltwb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "from sklearn.preprocessing import OneHotEncoder\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist"
      ],
      "metadata": {
        "id": "q-dv35vRV0uX"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf. __version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyRs45Tfy8Un",
        "outputId": "f31fb610-904e-475d-ddeb-e331e9559d77"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#[Problème 1] Retour sur le scratch\n",
        "\n",
        "Les étapes utiliser dans le scratch sont: \n",
        "- 1 - Initialisation du poids\n",
        "- 2 - utiliser une booucle d'epoque pour \n",
        "- 3 - Multiplier le poids par l'entrée\n",
        "- 4 - appliquer une fonction d'activation\n",
        "- 5 - extimer et mettre à jour les poids\n",
        "\n",
        "Mise e oeuvre avec tensorflow\n",
        "- Initialisation des poids :\n",
        "- Entrainement avec la boucle epoch\n",
        "\n",
        "```\n",
        "for epoch in range(1000):\n",
        "    sess.run(train_step, feed_dict={\n",
        "        x:x_train,\n",
        "        t:y_train\n",
        "    })\n",
        "```\n",
        "- Fonction d'activation\n",
        "```\n",
        "y = tf.sigmoid(tf.matmul(x, W) + b)\n",
        "```\n",
        "- Evaluation \n",
        "```\n",
        "acc_val = sess.run(accuracy,\n",
        "     feed_dict={\n",
        "        x:x_train,\n",
        "        t:y_train\n",
        "    })\n",
        "```\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "WO-le4ezoJDW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#[Problème 2] Considérez la correspondance entre le scratch et TensorFlow"
      ],
      "metadata": {
        "id": "72HGeZ-2tD8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "    ミニバッチを取得するイテレータ\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 次の形のndarray, shape (n_samples, n_features)\n",
        "      訓練データ\n",
        "    y : 次の形のndarray, shape (n_samples, 1)\n",
        "      正解値\n",
        "    batch_size : int\n",
        "      バッチサイズ\n",
        "    seed : int\n",
        "      NumPyの乱数のシード\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]"
      ],
      "metadata": {
        "id": "grDaAavNPBSx"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Classification binaire de l'ensemble de données Iris à l'aide d'un réseau de neurones implémenté dans TensorFlow\n",
        "\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "tf.test.gpu_device_name() \n",
        "\"\"\"\n",
        "tensorflowのバージョンを1.x系に変更した際は忘れずに\n",
        "「!pip install tensorflow-gpu==1.14.0」でGPUのインストールをしておきましょう。\n",
        "tf.test.gpu_device_name()でGPUの設定状態を確認し、認識されるかを確認します。\n",
        "成功している場合はログが出力されます、認識されない場合は何も出力されません。\n",
        "\"\"\"\n",
        "\n",
        "# データセットの読み込み\n",
        "df = pd.read_csv(\"Iris.csv\")\n",
        "\n",
        "# データフレームから条件抽出\n",
        "df = df[(df[\"Species\"] == \"Iris-versicolor\") | (df[\"Species\"] == \"Iris-virginica\")]\n",
        "y = df[\"Species\"]\n",
        "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
        "\n",
        "# NumPy 配列に変換\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "# ラベルを数値に変換\n",
        "y[y == \"Iris-versicolor\"] = 0\n",
        "y[y == \"Iris-virginica\"] = 1\n",
        "y = y.astype(np.int64)[:, np.newaxis]\n",
        "\n",
        "# trainとtestに分割\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "# さらにtrainとvalに分割\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "\n",
        "\n",
        "# ハイパーパラメータの設定\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 1\n",
        "\n",
        "# 計算グラフに渡す引数の形を決める\n",
        "X = tf.placeholder(dtype=\"float\", shape=[None, n_input])\n",
        "Y = tf.placeholder(dtype=\"float\", shape=[None, n_classes])\n",
        "\n",
        "# trainのミニバッチイテレータ\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "def example_net(x):\n",
        "    \"\"\"\n",
        "    単純な3層ニューラルネットワーク\n",
        "    \"\"\"\n",
        "    tf.random.set_random_seed(0)\n",
        "    # 重みとバイアスの宣言\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
        "    return layer_output\n",
        "\n",
        "# ネットワーク構造の読み込み                               \n",
        "logits = example_net(X)\n",
        "\n",
        "# 目的関数\n",
        "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
        "# 最適化手法\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "# 推定結果\n",
        "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
        "# 指標値計算\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# variableの初期化\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "\n",
        "# 計算グラフの実行\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        # エポックごとにループ\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # ミニバッチごとにループ\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "        total_loss /= n_samples\n",
        "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRoJnwbwoGTO",
        "outputId": "712b5a91-ddb0-4b6f-9e41-3679cca0f2a2"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 14.4403, val_loss : 81.6895, acc : 0.625\n",
            "Epoch 1, loss : 10.5877, val_loss : 57.9192, acc : 0.625\n",
            "Epoch 2, loss : 6.7000, val_loss : 33.6513, acc : 0.625\n",
            "Epoch 3, loss : 3.0198, val_loss : 10.7163, acc : 0.688\n",
            "Epoch 4, loss : 0.6364, val_loss : 5.7109, acc : 0.562\n",
            "Epoch 5, loss : 0.7567, val_loss : 11.6263, acc : 0.438\n",
            "Epoch 6, loss : 0.5221, val_loss : 4.5032, acc : 0.688\n",
            "Epoch 7, loss : 0.2289, val_loss : 2.7326, acc : 0.625\n",
            "Epoch 8, loss : 0.2752, val_loss : 2.4822, acc : 0.688\n",
            "Epoch 9, loss : 0.2005, val_loss : 2.7231, acc : 0.750\n",
            "Epoch 10, loss : 0.1875, val_loss : 2.6312, acc : 0.750\n",
            "Epoch 11, loss : 0.1578, val_loss : 2.2775, acc : 0.688\n",
            "Epoch 12, loss : 0.1575, val_loss : 2.1782, acc : 0.688\n",
            "Epoch 13, loss : 0.1519, val_loss : 2.1734, acc : 0.688\n",
            "Epoch 14, loss : 0.1433, val_loss : 2.1291, acc : 0.688\n",
            "Epoch 15, loss : 0.1350, val_loss : 2.0070, acc : 0.688\n",
            "Epoch 16, loss : 0.1308, val_loss : 1.9295, acc : 0.688\n",
            "Epoch 17, loss : 0.1272, val_loss : 1.8832, acc : 0.688\n",
            "Epoch 18, loss : 0.1220, val_loss : 1.8358, acc : 0.688\n",
            "Epoch 19, loss : 0.1171, val_loss : 1.7785, acc : 0.688\n",
            "Epoch 20, loss : 0.1135, val_loss : 1.7240, acc : 0.688\n",
            "Epoch 21, loss : 0.1101, val_loss : 1.6781, acc : 0.688\n",
            "Epoch 22, loss : 0.1065, val_loss : 1.6331, acc : 0.688\n",
            "Epoch 23, loss : 0.1029, val_loss : 1.5874, acc : 0.688\n",
            "Epoch 24, loss : 0.0995, val_loss : 1.5428, acc : 0.688\n",
            "Epoch 25, loss : 0.0961, val_loss : 1.4999, acc : 0.688\n",
            "Epoch 26, loss : 0.0930, val_loss : 1.4588, acc : 0.688\n",
            "Epoch 27, loss : 0.0897, val_loss : 1.4148, acc : 0.688\n",
            "Epoch 28, loss : 0.0864, val_loss : 1.3704, acc : 0.688\n",
            "Epoch 29, loss : 0.0831, val_loss : 1.3240, acc : 0.688\n",
            "Epoch 30, loss : 0.0799, val_loss : 1.2784, acc : 0.688\n",
            "Epoch 31, loss : 0.0767, val_loss : 1.2347, acc : 0.688\n",
            "Epoch 32, loss : 0.0736, val_loss : 1.1899, acc : 0.688\n",
            "Epoch 33, loss : 0.0708, val_loss : 1.1464, acc : 0.688\n",
            "Epoch 34, loss : 0.0682, val_loss : 1.1053, acc : 0.688\n",
            "Epoch 35, loss : 0.0657, val_loss : 1.0667, acc : 0.688\n",
            "Epoch 36, loss : 0.0633, val_loss : 1.0268, acc : 0.688\n",
            "Epoch 37, loss : 0.0612, val_loss : 0.9903, acc : 0.688\n",
            "Epoch 38, loss : 0.0593, val_loss : 0.9567, acc : 0.688\n",
            "Epoch 39, loss : 0.0576, val_loss : 0.9249, acc : 0.688\n",
            "Epoch 40, loss : 0.0560, val_loss : 0.8959, acc : 0.688\n",
            "Epoch 41, loss : 0.0546, val_loss : 0.8670, acc : 0.688\n",
            "Epoch 42, loss : 0.0532, val_loss : 0.8385, acc : 0.688\n",
            "Epoch 43, loss : 0.0519, val_loss : 0.8105, acc : 0.688\n",
            "Epoch 44, loss : 0.0506, val_loss : 0.7847, acc : 0.688\n",
            "Epoch 45, loss : 0.0494, val_loss : 0.7583, acc : 0.688\n",
            "Epoch 46, loss : 0.0480, val_loss : 0.7312, acc : 0.750\n",
            "Epoch 47, loss : 0.0462, val_loss : 0.7025, acc : 0.750\n",
            "Epoch 48, loss : 0.0442, val_loss : 0.6696, acc : 0.750\n",
            "Epoch 49, loss : 0.0422, val_loss : 0.6229, acc : 0.750\n",
            "Epoch 50, loss : 0.0402, val_loss : 0.5786, acc : 0.812\n",
            "Epoch 51, loss : 0.0382, val_loss : 0.5386, acc : 0.812\n",
            "Epoch 52, loss : 0.0361, val_loss : 0.5032, acc : 0.812\n",
            "Epoch 53, loss : 0.0341, val_loss : 0.4697, acc : 0.812\n",
            "Epoch 54, loss : 0.0320, val_loss : 0.4183, acc : 0.812\n",
            "Epoch 55, loss : 0.0300, val_loss : 0.3706, acc : 0.812\n",
            "Epoch 56, loss : 0.0282, val_loss : 0.3276, acc : 0.812\n",
            "Epoch 57, loss : 0.0265, val_loss : 0.2914, acc : 0.812\n",
            "Epoch 58, loss : 0.0249, val_loss : 0.2601, acc : 0.812\n",
            "Epoch 59, loss : 0.0233, val_loss : 0.2316, acc : 0.875\n",
            "Epoch 60, loss : 0.0219, val_loss : 0.2057, acc : 0.875\n",
            "Epoch 61, loss : 0.0207, val_loss : 0.1829, acc : 0.938\n",
            "Epoch 62, loss : 0.0196, val_loss : 0.1632, acc : 0.938\n",
            "Epoch 63, loss : 0.0187, val_loss : 0.1460, acc : 0.938\n",
            "Epoch 64, loss : 0.0178, val_loss : 0.1309, acc : 0.938\n",
            "Epoch 65, loss : 0.0170, val_loss : 0.1179, acc : 0.938\n",
            "Epoch 66, loss : 0.0163, val_loss : 0.1068, acc : 0.938\n",
            "Epoch 67, loss : 0.0157, val_loss : 0.0973, acc : 0.938\n",
            "Epoch 68, loss : 0.0153, val_loss : 0.0915, acc : 0.938\n",
            "Epoch 69, loss : 0.0149, val_loss : 0.0875, acc : 1.000\n",
            "Epoch 70, loss : 0.0146, val_loss : 0.0845, acc : 1.000\n",
            "Epoch 71, loss : 0.0143, val_loss : 0.0822, acc : 1.000\n",
            "Epoch 72, loss : 0.0141, val_loss : 0.0803, acc : 1.000\n",
            "Epoch 73, loss : 0.0138, val_loss : 0.0787, acc : 1.000\n",
            "Epoch 74, loss : 0.0135, val_loss : 0.0775, acc : 1.000\n",
            "Epoch 75, loss : 0.0133, val_loss : 0.0764, acc : 1.000\n",
            "Epoch 76, loss : 0.0130, val_loss : 0.0757, acc : 1.000\n",
            "Epoch 77, loss : 0.0128, val_loss : 0.0753, acc : 1.000\n",
            "Epoch 78, loss : 0.0125, val_loss : 0.0753, acc : 1.000\n",
            "Epoch 79, loss : 0.0123, val_loss : 0.0755, acc : 1.000\n",
            "Epoch 80, loss : 0.0120, val_loss : 0.0759, acc : 0.938\n",
            "Epoch 81, loss : 0.0118, val_loss : 0.0764, acc : 0.938\n",
            "Epoch 82, loss : 0.0116, val_loss : 0.0773, acc : 0.938\n",
            "Epoch 83, loss : 0.0113, val_loss : 0.0783, acc : 0.938\n",
            "Epoch 84, loss : 0.0111, val_loss : 0.0799, acc : 0.938\n",
            "Epoch 85, loss : 0.0109, val_loss : 0.0827, acc : 0.938\n",
            "Epoch 86, loss : 0.0107, val_loss : 0.0857, acc : 0.938\n",
            "Epoch 87, loss : 0.0105, val_loss : 0.0888, acc : 0.938\n",
            "Epoch 88, loss : 0.0102, val_loss : 0.0919, acc : 0.938\n",
            "Epoch 89, loss : 0.0100, val_loss : 0.0952, acc : 0.938\n",
            "Epoch 90, loss : 0.0098, val_loss : 0.0970, acc : 0.938\n",
            "Epoch 91, loss : 0.0096, val_loss : 0.0994, acc : 0.938\n",
            "Epoch 92, loss : 0.0094, val_loss : 0.1023, acc : 0.938\n",
            "Epoch 93, loss : 0.0092, val_loss : 0.1054, acc : 0.938\n",
            "Epoch 94, loss : 0.0090, val_loss : 0.1086, acc : 0.938\n",
            "Epoch 95, loss : 0.0088, val_loss : 0.1098, acc : 0.938\n",
            "Epoch 96, loss : 0.0086, val_loss : 0.1124, acc : 0.938\n",
            "Epoch 97, loss : 0.0084, val_loss : 0.1151, acc : 0.938\n",
            "Epoch 98, loss : 0.0082, val_loss : 0.1183, acc : 0.938\n",
            "Epoch 99, loss : 0.0080, val_loss : 0.1217, acc : 0.938\n",
            "test_acc : 0.900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#[Problème 3] Créez un modèle d'Iris en utilisant les trois variables objectives."
      ],
      "metadata": {
        "id": "gnRy31AP_Gbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"Iris.csv\")\n",
        "y = df[\"Species\"]\n",
        "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
        "\n",
        "# NumPy \n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "# \n",
        "y[y == 'Iris-setosa'] = 0\n",
        "y[y == \"Iris-versicolor\"] = 1\n",
        "y[y == \"Iris-virginica\"] = 2\n",
        "\n",
        "y = y.astype(np.int64)[:, np.newaxis]\n",
        "\n",
        "#one-hotエンコーディング\n",
        "enc = OneHotEncoder(categories='auto')\n",
        "y = enc.fit_transform(y).A\n",
        "\n",
        "\n",
        "\n",
        "# trainとtestに分割\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "# さらにtrainとvalに分割\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "# ハイパーパラメータの設定\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 3\n",
        "\n",
        "# \n",
        "X = tf.placeholder(dtype=\"float\", shape=[None, n_input])\n",
        "Y = tf.placeholder(dtype=\"float\", shape=[None, n_classes])\n",
        "\n",
        "# train\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "def example_net(x):\n",
        "\n",
        "    tf.random.set_random_seed(0)\n",
        "    # \n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3']\n",
        "    return layer_output\n",
        "\n",
        "# ネットワーク構造の読み込み                               \n",
        "logits = example_net(X)\n",
        "\n",
        "# 目的関数\n",
        "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
        "# 最適化手法\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "# 推定結果\n",
        "correct_pred = tf.equal(tf.argmax(Y, axis=1), tf.argmax(tf.nn.softmax(logits), axis=1))\n",
        "# 指標値計算\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# variableの初期化\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "\n",
        "# \n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # ミニバッチごとにループ\n",
        "            sess.run(train_op, feed_dict={X: X_train, Y: y_train})\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "\n",
        "        total_loss /= n_samples\n",
        "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzRk1E5n_GAm",
        "outputId": "563a5c61-b4c1-4a72-9ce0-c070b154d251"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 5.3173, val_loss : 31.7638, acc : 0.333\n",
            "Epoch 1, loss : 1.6568, val_loss : 2.7251, acc : 0.542\n",
            "Epoch 2, loss : 0.4273, val_loss : 1.6863, acc : 0.708\n",
            "Epoch 3, loss : 0.1414, val_loss : 0.5362, acc : 0.875\n",
            "Epoch 4, loss : 0.0957, val_loss : 0.4885, acc : 0.875\n",
            "Epoch 5, loss : 0.0727, val_loss : 0.4696, acc : 0.875\n",
            "Epoch 6, loss : 0.0570, val_loss : 0.4219, acc : 0.875\n",
            "Epoch 7, loss : 0.0432, val_loss : 0.3696, acc : 0.875\n",
            "Epoch 8, loss : 0.0308, val_loss : 0.3146, acc : 0.875\n",
            "Epoch 9, loss : 0.0209, val_loss : 0.2740, acc : 0.875\n",
            "Epoch 10, loss : 0.0150, val_loss : 0.2521, acc : 0.875\n",
            "Epoch 11, loss : 0.0112, val_loss : 0.2408, acc : 0.875\n",
            "Epoch 12, loss : 0.0085, val_loss : 0.2314, acc : 0.875\n",
            "Epoch 13, loss : 0.0066, val_loss : 0.2230, acc : 0.875\n",
            "Epoch 14, loss : 0.0054, val_loss : 0.2160, acc : 0.917\n",
            "Epoch 15, loss : 0.0046, val_loss : 0.2130, acc : 0.917\n",
            "Epoch 16, loss : 0.0041, val_loss : 0.2134, acc : 0.958\n",
            "Epoch 17, loss : 0.0039, val_loss : 0.2121, acc : 0.958\n",
            "Epoch 18, loss : 0.0037, val_loss : 0.2100, acc : 0.958\n",
            "Epoch 19, loss : 0.0035, val_loss : 0.2040, acc : 0.958\n",
            "Epoch 20, loss : 0.0032, val_loss : 0.1961, acc : 0.958\n",
            "Epoch 21, loss : 0.0029, val_loss : 0.1901, acc : 0.958\n",
            "Epoch 22, loss : 0.0026, val_loss : 0.1867, acc : 0.958\n",
            "Epoch 23, loss : 0.0024, val_loss : 0.1847, acc : 0.958\n",
            "Epoch 24, loss : 0.0023, val_loss : 0.1835, acc : 0.958\n",
            "Epoch 25, loss : 0.0022, val_loss : 0.1828, acc : 0.958\n",
            "Epoch 26, loss : 0.0021, val_loss : 0.1824, acc : 0.958\n",
            "Epoch 27, loss : 0.0020, val_loss : 0.1828, acc : 0.958\n",
            "Epoch 28, loss : 0.0019, val_loss : 0.1836, acc : 0.958\n",
            "Epoch 29, loss : 0.0018, val_loss : 0.1856, acc : 0.958\n",
            "Epoch 30, loss : 0.0017, val_loss : 0.1873, acc : 0.958\n",
            "Epoch 31, loss : 0.0017, val_loss : 0.1879, acc : 0.958\n",
            "Epoch 32, loss : 0.0016, val_loss : 0.1874, acc : 0.958\n",
            "Epoch 33, loss : 0.0015, val_loss : 0.1857, acc : 0.958\n",
            "Epoch 34, loss : 0.0015, val_loss : 0.1840, acc : 0.958\n",
            "Epoch 35, loss : 0.0014, val_loss : 0.1818, acc : 0.958\n",
            "Epoch 36, loss : 0.0014, val_loss : 0.1788, acc : 0.958\n",
            "Epoch 37, loss : 0.0013, val_loss : 0.1745, acc : 0.958\n",
            "Epoch 38, loss : 0.0012, val_loss : 0.1692, acc : 0.958\n",
            "Epoch 39, loss : 0.0012, val_loss : 0.1627, acc : 0.958\n",
            "Epoch 40, loss : 0.0012, val_loss : 0.1564, acc : 0.917\n",
            "Epoch 41, loss : 0.0012, val_loss : 0.1533, acc : 0.917\n",
            "Epoch 42, loss : 0.0013, val_loss : 0.1586, acc : 0.833\n",
            "Epoch 43, loss : 0.0014, val_loss : 0.1695, acc : 0.833\n",
            "Epoch 44, loss : 0.0015, val_loss : 0.1786, acc : 0.833\n",
            "Epoch 45, loss : 0.0016, val_loss : 0.1840, acc : 0.833\n",
            "Epoch 46, loss : 0.0016, val_loss : 0.1885, acc : 0.833\n",
            "Epoch 47, loss : 0.0016, val_loss : 0.1914, acc : 0.833\n",
            "Epoch 48, loss : 0.0016, val_loss : 0.1946, acc : 0.833\n",
            "Epoch 49, loss : 0.0016, val_loss : 0.1969, acc : 0.833\n",
            "Epoch 50, loss : 0.0016, val_loss : 0.1992, acc : 0.833\n",
            "Epoch 51, loss : 0.0015, val_loss : 0.2015, acc : 0.833\n",
            "Epoch 52, loss : 0.0015, val_loss : 0.2036, acc : 0.833\n",
            "Epoch 53, loss : 0.0015, val_loss : 0.2056, acc : 0.833\n",
            "Epoch 54, loss : 0.0015, val_loss : 0.2075, acc : 0.833\n",
            "Epoch 55, loss : 0.0015, val_loss : 0.2093, acc : 0.833\n",
            "Epoch 56, loss : 0.0015, val_loss : 0.2110, acc : 0.833\n",
            "Epoch 57, loss : 0.0014, val_loss : 0.2127, acc : 0.833\n",
            "Epoch 58, loss : 0.0014, val_loss : 0.2142, acc : 0.833\n",
            "Epoch 59, loss : 0.0014, val_loss : 0.2157, acc : 0.833\n",
            "Epoch 60, loss : 0.0014, val_loss : 0.2172, acc : 0.833\n",
            "Epoch 61, loss : 0.0014, val_loss : 0.2186, acc : 0.833\n",
            "Epoch 62, loss : 0.0014, val_loss : 0.2200, acc : 0.833\n",
            "Epoch 63, loss : 0.0014, val_loss : 0.2213, acc : 0.833\n",
            "Epoch 64, loss : 0.0014, val_loss : 0.2226, acc : 0.833\n",
            "Epoch 65, loss : 0.0014, val_loss : 0.2239, acc : 0.833\n",
            "Epoch 66, loss : 0.0013, val_loss : 0.2251, acc : 0.833\n",
            "Epoch 67, loss : 0.0013, val_loss : 0.2264, acc : 0.833\n",
            "Epoch 68, loss : 0.0013, val_loss : 0.2276, acc : 0.833\n",
            "Epoch 69, loss : 0.0013, val_loss : 0.2288, acc : 0.833\n",
            "Epoch 70, loss : 0.0013, val_loss : 0.2301, acc : 0.833\n",
            "Epoch 71, loss : 0.0013, val_loss : 0.2313, acc : 0.833\n",
            "Epoch 72, loss : 0.0013, val_loss : 0.2325, acc : 0.833\n",
            "Epoch 73, loss : 0.0013, val_loss : 0.2336, acc : 0.833\n",
            "Epoch 74, loss : 0.0013, val_loss : 0.2348, acc : 0.833\n",
            "Epoch 75, loss : 0.0013, val_loss : 0.2361, acc : 0.833\n",
            "Epoch 76, loss : 0.0013, val_loss : 0.2374, acc : 0.833\n",
            "Epoch 77, loss : 0.0013, val_loss : 0.2386, acc : 0.833\n",
            "Epoch 78, loss : 0.0013, val_loss : 0.2401, acc : 0.833\n",
            "Epoch 79, loss : 0.0013, val_loss : 0.2415, acc : 0.833\n",
            "Epoch 80, loss : 0.0012, val_loss : 0.2429, acc : 0.833\n",
            "Epoch 81, loss : 0.0012, val_loss : 0.2443, acc : 0.833\n",
            "Epoch 82, loss : 0.0012, val_loss : 0.2456, acc : 0.833\n",
            "Epoch 83, loss : 0.0012, val_loss : 0.2470, acc : 0.833\n",
            "Epoch 84, loss : 0.0012, val_loss : 0.2485, acc : 0.833\n",
            "Epoch 85, loss : 0.0012, val_loss : 0.2499, acc : 0.833\n",
            "Epoch 86, loss : 0.0012, val_loss : 0.2515, acc : 0.833\n",
            "Epoch 87, loss : 0.0012, val_loss : 0.2527, acc : 0.833\n",
            "Epoch 88, loss : 0.0012, val_loss : 0.2543, acc : 0.833\n",
            "Epoch 89, loss : 0.0012, val_loss : 0.2559, acc : 0.833\n",
            "Epoch 90, loss : 0.0012, val_loss : 0.2573, acc : 0.833\n",
            "Epoch 91, loss : 0.0012, val_loss : 0.2590, acc : 0.833\n",
            "Epoch 92, loss : 0.0012, val_loss : 0.2603, acc : 0.833\n",
            "Epoch 93, loss : 0.0012, val_loss : 0.2620, acc : 0.833\n",
            "Epoch 94, loss : 0.0012, val_loss : 0.2637, acc : 0.833\n",
            "Epoch 95, loss : 0.0012, val_loss : 0.2652, acc : 0.833\n",
            "Epoch 96, loss : 0.0012, val_loss : 0.2670, acc : 0.833\n",
            "Epoch 97, loss : 0.0012, val_loss : 0.2684, acc : 0.833\n",
            "Epoch 98, loss : 0.0012, val_loss : 0.2703, acc : 0.833\n",
            "Epoch 99, loss : 0.0012, val_loss : 0.2718, acc : 0.833\n",
            "test_acc : 0.967\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#[Problème 4] Création d'un modèle de prix des logements"
      ],
      "metadata": {
        "id": "gw88y1RZKLcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"train.csv\")\n",
        "y = df['SalePrice']\n",
        "X = df.loc[:, ['GrLivArea', 'YearBuilt']]\n",
        "\n",
        "# NumPy \n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "# \n",
        "y = y.astype(np.int64)[:, np.newaxis]\n",
        "\n",
        "# trainとtestに分割\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "# さらにtrainとvalに分割\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "# ハイパーパラメータの設定\n",
        "learning_rate = 0.005\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 1\n",
        "\n",
        "# \n",
        "X = tf.placeholder(dtype=\"float\", shape=[None, n_input])\n",
        "Y = tf.placeholder(dtype=\"float\", shape=[None, n_classes])\n",
        "\n",
        "# train\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "def example_net(x):\n",
        "\n",
        "    tf.random.set_random_seed(0)\n",
        "    # \n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3']\n",
        "    return layer_output\n",
        "\n",
        "                            \n",
        "logits = example_net(X)\n",
        "\n",
        "# 目的関数\n",
        "loss_op = tf.reduce_mean(tf.square(Y - logits))\n",
        "# 最適化手法\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "#\n",
        "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
        "#\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# variableの初期化\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "\n",
        "# \n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # ミニバッチごとにループ\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "        total_loss /= n_samples\n",
        "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoCfO0yBKJZu",
        "outputId": "5d5707a0-769c-4012-9804-f421135cead8"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 424048461.9101, val_loss : 2542237440.0000, acc : 1.000\n",
            "Epoch 1, loss : 313543446.1670, val_loss : 2528067328.0000, acc : 1.000\n",
            "Epoch 2, loss : 312923181.1906, val_loss : 2515195904.0000, acc : 1.000\n",
            "Epoch 3, loss : 312321907.2891, val_loss : 2508679168.0000, acc : 1.000\n",
            "Epoch 4, loss : 312325953.8844, val_loss : 2507447808.0000, acc : 1.000\n",
            "Epoch 5, loss : 312201289.1135, val_loss : 2506053888.0000, acc : 1.000\n",
            "Epoch 6, loss : 312164950.7495, val_loss : 2504360704.0000, acc : 1.000\n",
            "Epoch 7, loss : 311859163.8887, val_loss : 2506366720.0000, acc : 1.000\n",
            "Epoch 8, loss : 311742058.2441, val_loss : 2508734208.0000, acc : 1.000\n",
            "Epoch 9, loss : 311600691.0835, val_loss : 2511005184.0000, acc : 1.000\n",
            "Epoch 10, loss : 311752578.9122, val_loss : 2511408640.0000, acc : 1.000\n",
            "Epoch 11, loss : 311739755.5118, val_loss : 2514722304.0000, acc : 1.000\n",
            "Epoch 12, loss : 311712204.5396, val_loss : 2518148864.0000, acc : 1.000\n",
            "Epoch 13, loss : 312159971.0493, val_loss : 2508695296.0000, acc : 1.000\n",
            "Epoch 14, loss : 311148569.6274, val_loss : 2501074944.0000, acc : 1.000\n",
            "Epoch 15, loss : 311388010.7923, val_loss : 2517402880.0000, acc : 1.000\n",
            "Epoch 16, loss : 311940652.6424, val_loss : 2524413184.0000, acc : 1.000\n",
            "Epoch 17, loss : 312265127.0236, val_loss : 2524293120.0000, acc : 1.000\n",
            "Epoch 18, loss : 311940581.3448, val_loss : 2526536192.0000, acc : 1.000\n",
            "Epoch 19, loss : 311980289.8158, val_loss : 2529356032.0000, acc : 1.000\n",
            "Epoch 20, loss : 312110815.7944, val_loss : 2532883456.0000, acc : 1.000\n",
            "Epoch 21, loss : 312126583.6745, val_loss : 2537279488.0000, acc : 1.000\n",
            "Epoch 22, loss : 312412036.1113, val_loss : 2534876672.0000, acc : 1.000\n",
            "Epoch 23, loss : 312568764.5739, val_loss : 2537154816.0000, acc : 1.000\n",
            "Epoch 24, loss : 312567728.3769, val_loss : 2533818112.0000, acc : 1.000\n",
            "Epoch 25, loss : 312588918.0985, val_loss : 2532773376.0000, acc : 1.000\n",
            "Epoch 26, loss : 312590549.8929, val_loss : 2529420800.0000, acc : 1.000\n",
            "Epoch 27, loss : 312656130.2955, val_loss : 2528646144.0000, acc : 1.000\n",
            "Epoch 28, loss : 312757316.9336, val_loss : 2524500480.0000, acc : 1.000\n",
            "Epoch 29, loss : 312782332.7794, val_loss : 2520496128.0000, acc : 1.000\n",
            "Epoch 30, loss : 312934776.9764, val_loss : 2509352960.0000, acc : 1.000\n",
            "Epoch 31, loss : 312780239.5546, val_loss : 2514591488.0000, acc : 1.000\n",
            "Epoch 32, loss : 313157253.1734, val_loss : 2508015104.0000, acc : 1.000\n",
            "Epoch 33, loss : 313313950.5268, val_loss : 2501755904.0000, acc : 1.000\n",
            "Epoch 34, loss : 312892465.6103, val_loss : 2496076800.0000, acc : 1.000\n",
            "Epoch 35, loss : 312791446.6124, val_loss : 2494548736.0000, acc : 1.000\n",
            "Epoch 36, loss : 312578452.6595, val_loss : 2493324288.0000, acc : 1.000\n",
            "Epoch 37, loss : 312584683.1006, val_loss : 2491558656.0000, acc : 1.000\n",
            "Epoch 38, loss : 312499059.9400, val_loss : 2490795008.0000, acc : 1.000\n",
            "Epoch 39, loss : 312312553.0107, val_loss : 2493976320.0000, acc : 1.000\n",
            "Epoch 40, loss : 312618087.7430, val_loss : 2487830528.0000, acc : 1.000\n",
            "Epoch 41, loss : 312411983.1435, val_loss : 2484401152.0000, acc : 1.000\n",
            "Epoch 42, loss : 312451318.9893, val_loss : 2486000384.0000, acc : 1.000\n",
            "Epoch 43, loss : 312063028.9679, val_loss : 2490304768.0000, acc : 1.000\n",
            "Epoch 44, loss : 312165037.7045, val_loss : 2485911296.0000, acc : 1.000\n",
            "Epoch 45, loss : 312352667.1349, val_loss : 2480768768.0000, acc : 1.000\n",
            "Epoch 46, loss : 311895421.6017, val_loss : 2477891072.0000, acc : 1.000\n",
            "Epoch 47, loss : 311794808.0514, val_loss : 2485631744.0000, acc : 1.000\n",
            "Epoch 48, loss : 311793331.5632, val_loss : 2487503616.0000, acc : 1.000\n",
            "Epoch 49, loss : 311915687.1949, val_loss : 2484044544.0000, acc : 1.000\n",
            "Epoch 50, loss : 311841647.5889, val_loss : 2483976960.0000, acc : 1.000\n",
            "Epoch 51, loss : 311776908.6081, val_loss : 2483800576.0000, acc : 1.000\n",
            "Epoch 52, loss : 311770660.4540, val_loss : 2475087360.0000, acc : 1.000\n",
            "Epoch 53, loss : 311518000.5482, val_loss : 2473620480.0000, acc : 1.000\n",
            "Epoch 54, loss : 311242201.0792, val_loss : 2477301504.0000, acc : 1.000\n",
            "Epoch 55, loss : 311474635.5803, val_loss : 2476103168.0000, acc : 1.000\n",
            "Epoch 56, loss : 311291279.3833, val_loss : 2476838144.0000, acc : 1.000\n",
            "Epoch 57, loss : 311147539.3576, val_loss : 2474696192.0000, acc : 1.000\n",
            "Epoch 58, loss : 311303341.3619, val_loss : 2474278400.0000, acc : 1.000\n",
            "Epoch 59, loss : 310974789.2077, val_loss : 2476427520.0000, acc : 1.000\n",
            "Epoch 60, loss : 311285576.9422, val_loss : 2474243840.0000, acc : 1.000\n",
            "Epoch 61, loss : 311194869.7901, val_loss : 2471581952.0000, acc : 1.000\n",
            "Epoch 62, loss : 310859416.3255, val_loss : 2473062400.0000, acc : 1.000\n",
            "Epoch 63, loss : 311155532.0600, val_loss : 2471785216.0000, acc : 1.000\n",
            "Epoch 64, loss : 310799446.7495, val_loss : 2473921536.0000, acc : 1.000\n",
            "Epoch 65, loss : 311024798.1842, val_loss : 2472177152.0000, acc : 1.000\n",
            "Epoch 66, loss : 310799079.7430, val_loss : 2471116544.0000, acc : 1.000\n",
            "Epoch 67, loss : 310712278.7837, val_loss : 2471324928.0000, acc : 1.000\n",
            "Epoch 68, loss : 310960094.1499, val_loss : 2470606080.0000, acc : 1.000\n",
            "Epoch 69, loss : 310796466.1242, val_loss : 2471529728.0000, acc : 1.000\n",
            "Epoch 70, loss : 310767073.1991, val_loss : 2469676800.0000, acc : 1.000\n",
            "Epoch 71, loss : 310695937.1649, val_loss : 2470436352.0000, acc : 1.000\n",
            "Epoch 72, loss : 310823211.8201, val_loss : 2470657792.0000, acc : 1.000\n",
            "Epoch 73, loss : 310634329.8330, val_loss : 2470205952.0000, acc : 1.000\n",
            "Epoch 74, loss : 310546670.9036, val_loss : 2471840000.0000, acc : 1.000\n",
            "Epoch 75, loss : 310547915.7516, val_loss : 2474469888.0000, acc : 1.000\n",
            "Epoch 76, loss : 310649775.3490, val_loss : 2471762432.0000, acc : 1.000\n",
            "Epoch 77, loss : 310535310.4240, val_loss : 2471553280.0000, acc : 1.000\n",
            "Epoch 78, loss : 310606835.9400, val_loss : 2468580096.0000, acc : 1.000\n",
            "Epoch 79, loss : 310398669.4304, val_loss : 2469075968.0000, acc : 1.000\n",
            "Epoch 80, loss : 310454031.5546, val_loss : 2468517632.0000, acc : 1.000\n",
            "Epoch 81, loss : 310334569.1135, val_loss : 2468273920.0000, acc : 1.000\n",
            "Epoch 82, loss : 310394815.1777, val_loss : 2468921088.0000, acc : 1.000\n",
            "Epoch 83, loss : 310410247.1606, val_loss : 2468916992.0000, acc : 1.000\n",
            "Epoch 84, loss : 310406801.4047, val_loss : 2469850112.0000, acc : 1.000\n",
            "Epoch 85, loss : 310196783.4861, val_loss : 2469602560.0000, acc : 1.000\n",
            "Epoch 86, loss : 310174375.5032, val_loss : 2470712320.0000, acc : 1.000\n",
            "Epoch 87, loss : 309958549.4818, val_loss : 2469522432.0000, acc : 1.000\n",
            "Epoch 88, loss : 310195578.6210, val_loss : 2470050048.0000, acc : 1.000\n",
            "Epoch 89, loss : 309927075.5289, val_loss : 2470184192.0000, acc : 1.000\n",
            "Epoch 90, loss : 309879363.5289, val_loss : 2468464384.0000, acc : 1.000\n",
            "Epoch 91, loss : 309984074.2099, val_loss : 2469926656.0000, acc : 1.000\n",
            "Epoch 92, loss : 309868763.9914, val_loss : 2467776512.0000, acc : 1.000\n",
            "Epoch 93, loss : 309914108.5054, val_loss : 2469291264.0000, acc : 1.000\n",
            "Epoch 94, loss : 309822474.3469, val_loss : 2468217344.0000, acc : 1.000\n",
            "Epoch 95, loss : 310021023.5546, val_loss : 2471103232.0000, acc : 1.000\n",
            "Epoch 96, loss : 309667315.5974, val_loss : 2467517440.0000, acc : 1.000\n",
            "Epoch 97, loss : 309803644.7452, val_loss : 2468651008.0000, acc : 1.000\n",
            "Epoch 98, loss : 309699239.7773, val_loss : 2467971840.0000, acc : 1.000\n",
            "Epoch 99, loss : 309839696.5139, val_loss : 2468488960.0000, acc : 1.000\n",
            "test_acc : 1.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#[Problème 5] Création d'un modèle MNIST"
      ],
      "metadata": {
        "id": "8Smsq3jQVLxR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#データの読み込み\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "#次元変換\n",
        "y_train = y_train.astype(np.int)[:, np.newaxis]\n",
        "y_test = y_test.astype(np.int)[:, np.newaxis]\n",
        "\n",
        "#one-hotエンコーディング\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train = enc.fit_transform(y_train)\n",
        "y_test = enc.fit_transform(y_test)\n",
        "\n",
        "#データの変換\n",
        "X_train = X_train.reshape(-1, 784)\n",
        "X_test = X_test.reshape(-1, 784)\n",
        "X_train = X_train.astype(np.float)\n",
        "X_test = X_test.astype(np.float)\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "#分割\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
        "\n",
        "#ハイパーパラメータの設定\n",
        "learning_rate = 0.01 \n",
        "batch_size = 100 \n",
        "num_epochs = 10 \n",
        "n_hidden1 = 50 \n",
        "n_hidden2 = 25 \n",
        "n_input = X_train.shape[1] \n",
        "n_samples = X_train.shape[0] \n",
        "n_classes = 10 \n",
        "\n",
        "#計算グラフに渡す引数の形を決める\n",
        "X = tf.placeholder('float', [None, n_input])\n",
        "Y = tf.placeholder('float', [None, n_classes])\n",
        "\n",
        "#trainのミニバッチイテレータ\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "\n",
        "def example_net(x):\n",
        "    '''\n",
        "    単純な3層ニューラルネットワーク\n",
        "    '''\n",
        "    #重みとバイアスの宣言\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    \n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "    \n",
        "    #全結合層\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] #addと同じ\n",
        "    \n",
        "    return layer_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7z7QjdeqVM3H",
        "outputId": "9a6b1892-a21e-49cd-d76b-b9f93f62f18e"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ネットワーク構造の読み込み\n",
        "logits = example_net(X)\n",
        "\n",
        "#目的関数\n",
        "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
        "\n",
        "#最適化手法\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "#推定結果\n",
        "correct_pred = tf.equal(tf.argmax(Y, axis=1), tf.argmax(tf.nn.softmax(logits), axis=1))\n",
        "\n",
        "#指標値計算\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "#variableの初期値\n",
        "init = tf.global_variables_initializer()"
      ],
      "metadata": {
        "id": "LFN9XmSWVhZE"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # ミニバッチごとにループ\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "        total_loss /= n_samples\n",
        "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SA1qm9PdVsui",
        "outputId": "539b0fc1-ee43-4ffd-b49a-e34f8fabd30d"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 0.0111, val_loss : 0.1889, acc : 0.773\n",
            "Epoch 1, loss : 0.0013, val_loss : 0.1071, acc : 0.851\n",
            "Epoch 2, loss : 0.0009, val_loss : 0.0813, acc : 0.885\n",
            "Epoch 3, loss : 0.0007, val_loss : 0.0677, acc : 0.901\n",
            "Epoch 4, loss : 0.0006, val_loss : 0.0587, acc : 0.916\n",
            "Epoch 5, loss : 0.0005, val_loss : 0.0534, acc : 0.924\n",
            "Epoch 6, loss : 0.0004, val_loss : 0.0482, acc : 0.931\n",
            "Epoch 7, loss : 0.0004, val_loss : 0.0444, acc : 0.937\n",
            "Epoch 8, loss : 0.0003, val_loss : 0.0418, acc : 0.939\n",
            "Epoch 9, loss : 0.0003, val_loss : 0.0388, acc : 0.944\n",
            "test_acc : 0.945\n"
          ]
        }
      ]
    }
  ]
}