{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNHitRbkcwY2WKvPVF8cfhT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AngeMervaneJdev/ML_AI_1/blob/main/tf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==1.14"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjLlEMiz1BrC",
        "outputId": "8f97a84b-892f-4b01-c297-c34bf7196be3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==1.14\n",
            "  Downloading tensorflow-1.14.0-cp37-cp37m-manylinux1_x86_64.whl (109.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 109.3 MB 41 kB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.0.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.14.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.44.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (3.17.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (0.37.1)\n",
            "Collecting keras-applications>=1.0.6\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 3.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.21.5)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "  Downloading tensorboard-1.14.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 21.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (0.8.1)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "  Downloading tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488 kB)\n",
            "\u001b[K     |████████████████████████████████| 488 kB 41.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (0.5.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.1.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14) (1.15.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14) (3.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.3.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.10.0.2)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.6->tensorflow==1.14) (1.5.2)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.0\n",
            "    Uninstalling tensorflow-2.8.0:\n",
            "      Successfully uninstalled tensorflow-2.8.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.14.0 which is incompatible.\u001b[0m\n",
            "Successfully installed keras-applications-1.0.8 tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 1.x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHfoAhbR4EZd",
        "outputId": "f7533328-c343-42d2-bf2a-7653e1824314"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZDsqVacwltwb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dadc586-a37f-4390-8ccc-876895aaa217"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "from sklearn.preprocessing import OneHotEncoder\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist"
      ],
      "metadata": {
        "id": "q-dv35vRV0uX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "734eedad-65a7-4a9e-f3aa-18abfd749ca4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf. __version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyRs45Tfy8Un",
        "outputId": "4b9bc642-4e9e-4efa-e19d-8de0247214a8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.15.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#[Problème 1] Retour sur le scratch\n",
        "\n",
        "Les étapes utiliser dans le scratch sont: \n",
        "- 1 - Initialisation du poids\n",
        "- 2 - utiliser une booucle d'epoque pour \n",
        "- 3 - Multiplier le poids par l'entrée\n",
        "- 4 - appliquer une fonction d'activation\n",
        "- 5 - extimer et mettre à jour les poids\n",
        "\n",
        "Mise e oeuvre avec tensorflow\n",
        "- Initialisation des poids :\n",
        "- Entrainement avec la boucle epoch\n",
        "\n",
        "```\n",
        "for epoch in range(1000):\n",
        "    sess.run(train_step, feed_dict={\n",
        "        x:x_train,\n",
        "        t:y_train\n",
        "    })\n",
        "```\n",
        "- Fonction d'activation\n",
        "```\n",
        "y = tf.sigmoid(tf.matmul(x, W) + b)\n",
        "```\n",
        "- Evaluation \n",
        "```\n",
        "acc_val = sess.run(accuracy,\n",
        "     feed_dict={\n",
        "        x:x_train,\n",
        "        t:y_train\n",
        "    })\n",
        "```\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "WO-le4ezoJDW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#[Problème 2] Considérez la correspondance entre le scratch et TensorFlow"
      ],
      "metadata": {
        "id": "72HGeZ-2tD8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "    ミニバッチを取得するイテレータ\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 次の形のndarray, shape (n_samples, n_features)\n",
        "      訓練データ\n",
        "    y : 次の形のndarray, shape (n_samples, 1)\n",
        "      正解値\n",
        "    batch_size : int\n",
        "      バッチサイズ\n",
        "    seed : int\n",
        "      NumPyの乱数のシード\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]"
      ],
      "metadata": {
        "id": "grDaAavNPBSx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Classification binaire de l'ensemble de données Iris à l'aide d'un réseau de neurones implémenté dans TensorFlow\n",
        "\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "tf.test.gpu_device_name() \n",
        "\"\"\"\n",
        "tensorflowのバージョンを1.x系に変更した際は忘れずに\n",
        "「!pip install tensorflow-gpu==1.14.0」でGPUのインストールをしておきましょう。\n",
        "tf.test.gpu_device_name()でGPUの設定状態を確認し、認識されるかを確認します。\n",
        "成功している場合はログが出力されます、認識されない場合は何も出力されません。\n",
        "\"\"\"\n",
        "\n",
        "# データセットの読み込み\n",
        "df = pd.read_csv(\"Iris.csv\")\n",
        "\n",
        "# データフレームから条件抽出\n",
        "df = df[(df[\"Species\"] == \"Iris-versicolor\") | (df[\"Species\"] == \"Iris-virginica\")]\n",
        "y = df[\"Species\"]\n",
        "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
        "\n",
        "# NumPy 配列に変換\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "# ラベルを数値に変換\n",
        "y[y == \"Iris-versicolor\"] = 0\n",
        "y[y == \"Iris-virginica\"] = 1\n",
        "y = y.astype(np.int64)[:, np.newaxis]\n",
        "\n",
        "# trainとtestに分割\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "# さらにtrainとvalに分割\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "\n",
        "\n",
        "# ハイパーパラメータの設定\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 1\n",
        "\n",
        "# 計算グラフに渡す引数の形を決める\n",
        "X = tf.placeholder(dtype=\"float\", shape=[None, n_input])\n",
        "Y = tf.placeholder(dtype=\"float\", shape=[None, n_classes])\n",
        "\n",
        "# trainのミニバッチイテレータ\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "def example_net(x):\n",
        "    \"\"\"\n",
        "    単純な3層ニューラルネットワーク\n",
        "    \"\"\"\n",
        "    tf.random.set_random_seed(0)\n",
        "    # 重みとバイアスの宣言\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
        "    return layer_output\n",
        "\n",
        "# ネットワーク構造の読み込み                               \n",
        "logits = example_net(X)\n",
        "\n",
        "# 目的関数\n",
        "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
        "# 最適化手法\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "# 推定結果\n",
        "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
        "# 指標値計算\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# variableの初期化\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "\n",
        "# 計算グラフの実行\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        # エポックごとにループ\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # ミニバッチごとにループ\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "        total_loss /= n_samples\n",
        "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRoJnwbwoGTO",
        "outputId": "074fd4c8-dc1e-4e59-efa6-6f1e95d6835f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 5.4193, val_loss : 57.2231, acc : 0.375\n",
            "Epoch 1, loss : 3.5689, val_loss : 34.6412, acc : 0.375\n",
            "Epoch 2, loss : 2.1442, val_loss : 15.9005, acc : 0.188\n",
            "Epoch 3, loss : 1.7802, val_loss : 11.7099, acc : 0.062\n",
            "Epoch 4, loss : 1.4782, val_loss : 11.4397, acc : 0.188\n",
            "Epoch 5, loss : 1.2006, val_loss : 12.0541, acc : 0.250\n",
            "Epoch 6, loss : 1.0606, val_loss : 8.0987, acc : 0.188\n",
            "Epoch 7, loss : 0.9051, val_loss : 5.9181, acc : 0.312\n",
            "Epoch 8, loss : 0.7778, val_loss : 5.5638, acc : 0.312\n",
            "Epoch 9, loss : 0.6314, val_loss : 4.3611, acc : 0.312\n",
            "Epoch 10, loss : 0.4857, val_loss : 2.7496, acc : 0.438\n",
            "Epoch 11, loss : 0.3648, val_loss : 2.1538, acc : 0.438\n",
            "Epoch 12, loss : 0.2902, val_loss : 1.5546, acc : 0.438\n",
            "Epoch 13, loss : 0.2336, val_loss : 1.0378, acc : 0.562\n",
            "Epoch 14, loss : 0.1847, val_loss : 0.9605, acc : 0.688\n",
            "Epoch 15, loss : 0.1568, val_loss : 0.7830, acc : 0.688\n",
            "Epoch 16, loss : 0.1283, val_loss : 0.5272, acc : 0.812\n",
            "Epoch 17, loss : 0.1076, val_loss : 0.3977, acc : 0.875\n",
            "Epoch 18, loss : 0.0932, val_loss : 0.2476, acc : 0.875\n",
            "Epoch 19, loss : 0.0830, val_loss : 0.1727, acc : 0.938\n",
            "Epoch 20, loss : 0.0759, val_loss : 0.1069, acc : 0.938\n",
            "Epoch 21, loss : 0.0700, val_loss : 0.0713, acc : 1.000\n",
            "Epoch 22, loss : 0.0653, val_loss : 0.0530, acc : 1.000\n",
            "Epoch 23, loss : 0.0614, val_loss : 0.0443, acc : 1.000\n",
            "Epoch 24, loss : 0.0583, val_loss : 0.0419, acc : 1.000\n",
            "Epoch 25, loss : 0.0559, val_loss : 0.0413, acc : 1.000\n",
            "Epoch 26, loss : 0.0540, val_loss : 0.0416, acc : 1.000\n",
            "Epoch 27, loss : 0.0524, val_loss : 0.0417, acc : 1.000\n",
            "Epoch 28, loss : 0.0507, val_loss : 0.0417, acc : 1.000\n",
            "Epoch 29, loss : 0.0492, val_loss : 0.0412, acc : 1.000\n",
            "Epoch 30, loss : 0.0476, val_loss : 0.0403, acc : 1.000\n",
            "Epoch 31, loss : 0.0455, val_loss : 0.0394, acc : 1.000\n",
            "Epoch 32, loss : 0.0425, val_loss : 0.0329, acc : 1.000\n",
            "Epoch 33, loss : 0.0391, val_loss : 0.0293, acc : 1.000\n",
            "Epoch 34, loss : 0.0356, val_loss : 0.0289, acc : 1.000\n",
            "Epoch 35, loss : 0.0319, val_loss : 0.0310, acc : 1.000\n",
            "Epoch 36, loss : 0.0281, val_loss : 0.0372, acc : 1.000\n",
            "Epoch 37, loss : 0.0242, val_loss : 0.0494, acc : 1.000\n",
            "Epoch 38, loss : 0.0213, val_loss : 0.0600, acc : 1.000\n",
            "Epoch 39, loss : 0.0190, val_loss : 0.0637, acc : 1.000\n",
            "Epoch 40, loss : 0.0170, val_loss : 0.0598, acc : 1.000\n",
            "Epoch 41, loss : 0.0154, val_loss : 0.0547, acc : 1.000\n",
            "Epoch 42, loss : 0.0140, val_loss : 0.0473, acc : 1.000\n",
            "Epoch 43, loss : 0.0132, val_loss : 0.0516, acc : 1.000\n",
            "Epoch 44, loss : 0.0123, val_loss : 0.0554, acc : 1.000\n",
            "Epoch 45, loss : 0.0113, val_loss : 0.0501, acc : 1.000\n",
            "Epoch 46, loss : 0.0108, val_loss : 0.0510, acc : 1.000\n",
            "Epoch 47, loss : 0.0104, val_loss : 0.0549, acc : 1.000\n",
            "Epoch 48, loss : 0.0097, val_loss : 0.0518, acc : 1.000\n",
            "Epoch 49, loss : 0.0095, val_loss : 0.0531, acc : 1.000\n",
            "Epoch 50, loss : 0.0091, val_loss : 0.0561, acc : 1.000\n",
            "Epoch 51, loss : 0.0087, val_loss : 0.0541, acc : 1.000\n",
            "Epoch 52, loss : 0.0085, val_loss : 0.0568, acc : 1.000\n",
            "Epoch 53, loss : 0.0082, val_loss : 0.0581, acc : 1.000\n",
            "Epoch 54, loss : 0.0079, val_loss : 0.0567, acc : 1.000\n",
            "Epoch 55, loss : 0.0077, val_loss : 0.0585, acc : 1.000\n",
            "Epoch 56, loss : 0.0074, val_loss : 0.0592, acc : 1.000\n",
            "Epoch 57, loss : 0.0072, val_loss : 0.0592, acc : 0.938\n",
            "Epoch 58, loss : 0.0069, val_loss : 0.0599, acc : 0.938\n",
            "Epoch 59, loss : 0.0067, val_loss : 0.0595, acc : 0.938\n",
            "Epoch 60, loss : 0.0065, val_loss : 0.0614, acc : 0.938\n",
            "Epoch 61, loss : 0.0063, val_loss : 0.0609, acc : 0.938\n",
            "Epoch 62, loss : 0.0061, val_loss : 0.0619, acc : 0.938\n",
            "Epoch 63, loss : 0.0059, val_loss : 0.0614, acc : 0.938\n",
            "Epoch 64, loss : 0.0057, val_loss : 0.0627, acc : 0.938\n",
            "Epoch 65, loss : 0.0055, val_loss : 0.0625, acc : 0.938\n",
            "Epoch 66, loss : 0.0054, val_loss : 0.0632, acc : 0.938\n",
            "Epoch 67, loss : 0.0052, val_loss : 0.0624, acc : 0.938\n",
            "Epoch 68, loss : 0.0051, val_loss : 0.0638, acc : 0.938\n",
            "Epoch 69, loss : 0.0049, val_loss : 0.0629, acc : 0.938\n",
            "Epoch 70, loss : 0.0048, val_loss : 0.0639, acc : 0.938\n",
            "Epoch 71, loss : 0.0046, val_loss : 0.0627, acc : 0.938\n",
            "Epoch 72, loss : 0.0045, val_loss : 0.0639, acc : 0.938\n",
            "Epoch 73, loss : 0.0043, val_loss : 0.0628, acc : 0.938\n",
            "Epoch 74, loss : 0.0042, val_loss : 0.0638, acc : 0.938\n",
            "Epoch 75, loss : 0.0041, val_loss : 0.0634, acc : 0.938\n",
            "Epoch 76, loss : 0.0040, val_loss : 0.0639, acc : 0.938\n",
            "Epoch 77, loss : 0.0039, val_loss : 0.0638, acc : 0.938\n",
            "Epoch 78, loss : 0.0038, val_loss : 0.0639, acc : 0.938\n",
            "Epoch 79, loss : 0.0037, val_loss : 0.0638, acc : 0.938\n",
            "Epoch 80, loss : 0.0036, val_loss : 0.0632, acc : 0.938\n",
            "Epoch 81, loss : 0.0035, val_loss : 0.0637, acc : 0.938\n",
            "Epoch 82, loss : 0.0034, val_loss : 0.0627, acc : 0.938\n",
            "Epoch 83, loss : 0.0034, val_loss : 0.0632, acc : 0.938\n",
            "Epoch 84, loss : 0.0033, val_loss : 0.0623, acc : 0.938\n",
            "Epoch 85, loss : 0.0032, val_loss : 0.0625, acc : 0.938\n",
            "Epoch 86, loss : 0.0031, val_loss : 0.0619, acc : 0.938\n",
            "Epoch 87, loss : 0.0031, val_loss : 0.0619, acc : 0.938\n",
            "Epoch 88, loss : 0.0030, val_loss : 0.0613, acc : 0.938\n",
            "Epoch 89, loss : 0.0029, val_loss : 0.0612, acc : 0.938\n",
            "Epoch 90, loss : 0.0029, val_loss : 0.0608, acc : 0.938\n",
            "Epoch 91, loss : 0.0028, val_loss : 0.0605, acc : 0.938\n",
            "Epoch 92, loss : 0.0028, val_loss : 0.0601, acc : 0.938\n",
            "Epoch 93, loss : 0.0027, val_loss : 0.0599, acc : 0.938\n",
            "Epoch 94, loss : 0.0027, val_loss : 0.0595, acc : 0.938\n",
            "Epoch 95, loss : 0.0026, val_loss : 0.0592, acc : 0.938\n",
            "Epoch 96, loss : 0.0026, val_loss : 0.0588, acc : 0.938\n",
            "Epoch 97, loss : 0.0025, val_loss : 0.0584, acc : 0.938\n",
            "Epoch 98, loss : 0.0025, val_loss : 0.0581, acc : 0.938\n",
            "Epoch 99, loss : 0.0024, val_loss : 0.0577, acc : 0.938\n",
            "test_acc : 0.900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#[Problème 3] Créez un modèle d'Iris en utilisant les trois variables objectives."
      ],
      "metadata": {
        "id": "gnRy31AP_Gbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"Iris.csv\")\n",
        "y = df[\"Species\"]\n",
        "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
        "\n",
        "# NumPy \n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "# \n",
        "y[y == 'Iris-setosa'] = 0\n",
        "y[y == \"Iris-versicolor\"] = 1\n",
        "y[y == \"Iris-virginica\"] = 2\n",
        "\n",
        "y = y.astype(np.int64)[:, np.newaxis]\n",
        "\n",
        "#one-hotエンコーディング\n",
        "enc = OneHotEncoder(categories='auto')\n",
        "y = enc.fit_transform(y).A\n",
        "\n",
        "# trainとtestに分割\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "# さらにtrainとvalに分割\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "# ハイパーパラメータの設定\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 3\n",
        "\n",
        "# \n",
        "X = tf.placeholder(dtype=\"float\", shape=[None, n_input])\n",
        "Y = tf.placeholder(dtype=\"float\", shape=[None, n_classes])\n",
        "\n",
        "# train\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "def example_net(x):\n",
        "\n",
        "    tf.random.set_random_seed(0)\n",
        "    # \n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3']\n",
        "    return layer_output\n",
        "\n",
        "# ネットワーク構造の読み込み                               \n",
        "logits = example_net(X)\n",
        "\n",
        "# 目的関数\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
        "# 最適化手法\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "# 推定結果\n",
        "correct_pred = tf.equal(tf.argmax(Y, axis=1), tf.argmax(tf.nn.softmax(logits), axis=1))\n",
        "# 指標値計算\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# variableの初期化\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "\n",
        "# \n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # ミニバッチごとにループ\n",
        "            sess.run(train_op, feed_dict={X: X_train, Y: y_train})\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "\n",
        "        total_loss /= n_samples\n",
        "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzRk1E5n_GAm",
        "outputId": "458e5d14-63df-43a8-9f72-09fd284bd84f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 22.1617, val_loss : 150.2520, acc : 0.625\n",
            "Epoch 1, loss : 9.8478, val_loss : 32.4296, acc : 0.500\n",
            "Epoch 2, loss : 4.5410, val_loss : 19.4940, acc : 0.583\n",
            "Epoch 3, loss : 3.2280, val_loss : 15.0229, acc : 0.458\n",
            "Epoch 4, loss : 2.8753, val_loss : 11.2347, acc : 0.417\n",
            "Epoch 5, loss : 2.3603, val_loss : 9.9260, acc : 0.417\n",
            "Epoch 6, loss : 1.9581, val_loss : 8.8065, acc : 0.417\n",
            "Epoch 7, loss : 1.5335, val_loss : 7.0626, acc : 0.458\n",
            "Epoch 8, loss : 1.0305, val_loss : 4.8160, acc : 0.500\n",
            "Epoch 9, loss : 0.4991, val_loss : 1.6103, acc : 0.750\n",
            "Epoch 10, loss : 0.1910, val_loss : 0.6418, acc : 0.750\n",
            "Epoch 11, loss : 0.1086, val_loss : 0.6254, acc : 0.875\n",
            "Epoch 12, loss : 0.0648, val_loss : 0.6366, acc : 0.875\n",
            "Epoch 13, loss : 0.0414, val_loss : 0.6748, acc : 0.917\n",
            "Epoch 14, loss : 0.0245, val_loss : 0.9613, acc : 0.833\n",
            "Epoch 15, loss : 0.0173, val_loss : 1.1632, acc : 0.833\n",
            "Epoch 16, loss : 0.0122, val_loss : 1.1324, acc : 0.833\n",
            "Epoch 17, loss : 0.0090, val_loss : 1.0284, acc : 0.875\n",
            "Epoch 18, loss : 0.0069, val_loss : 0.9187, acc : 0.875\n",
            "Epoch 19, loss : 0.0055, val_loss : 0.8677, acc : 0.875\n",
            "Epoch 20, loss : 0.0045, val_loss : 0.8280, acc : 0.875\n",
            "Epoch 21, loss : 0.0038, val_loss : 0.7853, acc : 0.875\n",
            "Epoch 22, loss : 0.0032, val_loss : 0.7435, acc : 0.875\n",
            "Epoch 23, loss : 0.0028, val_loss : 0.7041, acc : 0.875\n",
            "Epoch 24, loss : 0.0025, val_loss : 0.6688, acc : 0.875\n",
            "Epoch 25, loss : 0.0022, val_loss : 0.6374, acc : 0.875\n",
            "Epoch 26, loss : 0.0020, val_loss : 0.6097, acc : 0.875\n",
            "Epoch 27, loss : 0.0019, val_loss : 0.5849, acc : 0.875\n",
            "Epoch 28, loss : 0.0017, val_loss : 0.5627, acc : 0.875\n",
            "Epoch 29, loss : 0.0016, val_loss : 0.5428, acc : 0.875\n",
            "Epoch 30, loss : 0.0015, val_loss : 0.5249, acc : 0.875\n",
            "Epoch 31, loss : 0.0014, val_loss : 0.5089, acc : 0.875\n",
            "Epoch 32, loss : 0.0013, val_loss : 0.4946, acc : 0.875\n",
            "Epoch 33, loss : 0.0012, val_loss : 0.4820, acc : 0.875\n",
            "Epoch 34, loss : 0.0012, val_loss : 0.4707, acc : 0.917\n",
            "Epoch 35, loss : 0.0011, val_loss : 0.4607, acc : 0.917\n",
            "Epoch 36, loss : 0.0011, val_loss : 0.4518, acc : 0.958\n",
            "Epoch 37, loss : 0.0010, val_loss : 0.4438, acc : 0.958\n",
            "Epoch 38, loss : 0.0010, val_loss : 0.4366, acc : 0.958\n",
            "Epoch 39, loss : 0.0009, val_loss : 0.4299, acc : 0.958\n",
            "Epoch 40, loss : 0.0009, val_loss : 0.4237, acc : 0.958\n",
            "Epoch 41, loss : 0.0009, val_loss : 0.4180, acc : 0.958\n",
            "Epoch 42, loss : 0.0008, val_loss : 0.4125, acc : 0.958\n",
            "Epoch 43, loss : 0.0008, val_loss : 0.4073, acc : 0.958\n",
            "Epoch 44, loss : 0.0008, val_loss : 0.4023, acc : 0.958\n",
            "Epoch 45, loss : 0.0008, val_loss : 0.3975, acc : 0.958\n",
            "Epoch 46, loss : 0.0007, val_loss : 0.3929, acc : 0.958\n",
            "Epoch 47, loss : 0.0007, val_loss : 0.3887, acc : 0.958\n",
            "Epoch 48, loss : 0.0007, val_loss : 0.3848, acc : 0.958\n",
            "Epoch 49, loss : 0.0007, val_loss : 0.3812, acc : 0.958\n",
            "Epoch 50, loss : 0.0007, val_loss : 0.3780, acc : 0.958\n",
            "Epoch 51, loss : 0.0007, val_loss : 0.3752, acc : 0.958\n",
            "Epoch 52, loss : 0.0007, val_loss : 0.3727, acc : 0.958\n",
            "Epoch 53, loss : 0.0007, val_loss : 0.3707, acc : 0.917\n",
            "Epoch 54, loss : 0.0007, val_loss : 0.3689, acc : 0.917\n",
            "Epoch 55, loss : 0.0006, val_loss : 0.3674, acc : 0.917\n",
            "Epoch 56, loss : 0.0006, val_loss : 0.3662, acc : 0.917\n",
            "Epoch 57, loss : 0.0006, val_loss : 0.3653, acc : 0.917\n",
            "Epoch 58, loss : 0.0006, val_loss : 0.3646, acc : 0.917\n",
            "Epoch 59, loss : 0.0006, val_loss : 0.3641, acc : 0.917\n",
            "Epoch 60, loss : 0.0006, val_loss : 0.3638, acc : 0.917\n",
            "Epoch 61, loss : 0.0006, val_loss : 0.3636, acc : 0.917\n",
            "Epoch 62, loss : 0.0006, val_loss : 0.3637, acc : 0.917\n",
            "Epoch 63, loss : 0.0005, val_loss : 0.3638, acc : 0.917\n",
            "Epoch 64, loss : 0.0005, val_loss : 0.3641, acc : 0.917\n",
            "Epoch 65, loss : 0.0005, val_loss : 0.3645, acc : 0.917\n",
            "Epoch 66, loss : 0.0005, val_loss : 0.3650, acc : 0.917\n",
            "Epoch 67, loss : 0.0005, val_loss : 0.3655, acc : 0.917\n",
            "Epoch 68, loss : 0.0005, val_loss : 0.3661, acc : 0.917\n",
            "Epoch 69, loss : 0.0005, val_loss : 0.3668, acc : 0.917\n",
            "Epoch 70, loss : 0.0005, val_loss : 0.3676, acc : 0.917\n",
            "Epoch 71, loss : 0.0004, val_loss : 0.3683, acc : 0.917\n",
            "Epoch 72, loss : 0.0004, val_loss : 0.3692, acc : 0.917\n",
            "Epoch 73, loss : 0.0004, val_loss : 0.3700, acc : 0.917\n",
            "Epoch 74, loss : 0.0004, val_loss : 0.3708, acc : 0.917\n",
            "Epoch 75, loss : 0.0004, val_loss : 0.3717, acc : 0.917\n",
            "Epoch 76, loss : 0.0004, val_loss : 0.3725, acc : 0.917\n",
            "Epoch 77, loss : 0.0004, val_loss : 0.3734, acc : 0.917\n",
            "Epoch 78, loss : 0.0004, val_loss : 0.3743, acc : 0.917\n",
            "Epoch 79, loss : 0.0004, val_loss : 0.3751, acc : 0.917\n",
            "Epoch 80, loss : 0.0004, val_loss : 0.3760, acc : 0.917\n",
            "Epoch 81, loss : 0.0003, val_loss : 0.3769, acc : 0.917\n",
            "Epoch 82, loss : 0.0003, val_loss : 0.3777, acc : 0.917\n",
            "Epoch 83, loss : 0.0003, val_loss : 0.3786, acc : 0.917\n",
            "Epoch 84, loss : 0.0003, val_loss : 0.3794, acc : 0.917\n",
            "Epoch 85, loss : 0.0003, val_loss : 0.3802, acc : 0.917\n",
            "Epoch 86, loss : 0.0003, val_loss : 0.3811, acc : 0.917\n",
            "Epoch 87, loss : 0.0003, val_loss : 0.3819, acc : 0.917\n",
            "Epoch 88, loss : 0.0003, val_loss : 0.3827, acc : 0.917\n",
            "Epoch 89, loss : 0.0003, val_loss : 0.3836, acc : 0.917\n",
            "Epoch 90, loss : 0.0003, val_loss : 0.3844, acc : 0.917\n",
            "Epoch 91, loss : 0.0003, val_loss : 0.3852, acc : 0.917\n",
            "Epoch 92, loss : 0.0003, val_loss : 0.3860, acc : 0.917\n",
            "Epoch 93, loss : 0.0003, val_loss : 0.3868, acc : 0.917\n",
            "Epoch 94, loss : 0.0003, val_loss : 0.3876, acc : 0.917\n",
            "Epoch 95, loss : 0.0002, val_loss : 0.3885, acc : 0.917\n",
            "Epoch 96, loss : 0.0002, val_loss : 0.3893, acc : 0.917\n",
            "Epoch 97, loss : 0.0002, val_loss : 0.3901, acc : 0.917\n",
            "Epoch 98, loss : 0.0002, val_loss : 0.3910, acc : 0.917\n",
            "Epoch 99, loss : 0.0002, val_loss : 0.3918, acc : 0.917\n",
            "test_acc : 0.967\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#[Problème 4] Création d'un modèle de prix des logements"
      ],
      "metadata": {
        "id": "gw88y1RZKLcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"train.csv\")\n",
        "y = df['SalePrice']\n",
        "X = df.loc[:, ['GrLivArea', 'YearBuilt']]\n",
        "\n",
        "# NumPy \n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "# \n",
        "y = y.astype(np.int64)[:, np.newaxis]\n",
        "\n",
        "# trainとtestに分割\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "# さらにtrainとvalに分割\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "# ハイパーパラメータの設定\n",
        "learning_rate = 0.005\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 1\n",
        "\n",
        "# \n",
        "X = tf.placeholder(dtype=\"float\", shape=[None, n_input])\n",
        "Y = tf.placeholder(dtype=\"float\", shape=[None, n_classes])\n",
        "\n",
        "# train\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "def example_net(x):\n",
        "\n",
        "    tf.random.set_random_seed(0)\n",
        "    # \n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3']\n",
        "    return layer_output\n",
        "\n",
        "                            \n",
        "logits = example_net(X)\n",
        "\n",
        "loss_op = tf.reduce_mean(tf.square(Y - logits))\n",
        "\n",
        "# 最適化手法\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "pred = logits\n",
        "\n",
        "mse = tf.reduce_mean(tf.square(Y - pred))\n",
        "r2 =  1 - (tf.reduce_sum(tf.square(Y - pred)) / tf.reduce_sum(tf.square(Y - tf.reduce_mean(Y))))\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# \n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # ミニバッチごとにループ\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "\n",
        "            loss = sess.run(loss_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "\n",
        "        total_loss /= n_samples\n",
        "        val_loss= sess.run(loss_op, feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n",
        "    \n",
        "    #estimation   \n",
        "    test_mse = sess.run(mse, feed_dict={X: X_test, Y: y_test})\n",
        "    test_r2 = sess.run(r2, feed_dict={X: X_test, Y: y_test})\n",
        "\n",
        "    print('test_mse : {:.3f}'.format(test_mse))\n",
        "    print('test_R2 : {:.3f}'.format(test_r2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoCfO0yBKJZu",
        "outputId": "7a041475-6009-4d8a-e8b2-e87c001d78bc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 1, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 2, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 3, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 4, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 5, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 6, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 7, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 8, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 9, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 10, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 11, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 12, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 13, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 14, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 15, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 16, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 17, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 18, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 19, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 20, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 21, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 22, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 23, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 24, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 25, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 26, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 27, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 28, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 29, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 30, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 31, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 32, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 33, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 34, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 35, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 36, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 37, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 38, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 39, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 40, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 41, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 42, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 43, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 44, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 45, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 46, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 47, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 48, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 49, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 50, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 51, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 52, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 53, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 54, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 55, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 56, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 57, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 58, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 59, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 60, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 61, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 62, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 63, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 64, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 65, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 66, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 67, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 68, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 69, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 70, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 71, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 72, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 73, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 74, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 75, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 76, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 77, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 78, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 79, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 80, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 81, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 82, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 83, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 84, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 85, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 86, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 87, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 88, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 89, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 90, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 91, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 92, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 93, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 94, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 95, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 96, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 97, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 98, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "Epoch 99, loss : 0.0000, val_loss : 0.0000, acc : 0.917\n",
            "test_mse : 2008729949329738755473408.000\n",
            "test_R2 : -290873707331584.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#[Problème 5] Création d'un modèle MNIST"
      ],
      "metadata": {
        "id": "8Smsq3jQVLxR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#データの読み込み\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "#次元変換\n",
        "y_train = y_train.astype(np.int)[:, np.newaxis]\n",
        "y_test = y_test.astype(np.int)[:, np.newaxis]\n",
        "\n",
        "#one-hotエンコーディング\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train = enc.fit_transform(y_train)\n",
        "y_test = enc.fit_transform(y_test)\n",
        "\n",
        "#データの変換\n",
        "X_train = X_train.reshape(-1, 784)\n",
        "X_test = X_test.reshape(-1, 784)\n",
        "X_train = X_train.astype(np.float)\n",
        "X_test = X_test.astype(np.float)\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "#分割\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
        "\n",
        "#ハイパーパラメータの設定\n",
        "learning_rate = 0.01 \n",
        "batch_size = 100 \n",
        "num_epochs = 10 \n",
        "n_hidden1 = 50 \n",
        "n_hidden2 = 25 \n",
        "n_input = X_train.shape[1] \n",
        "n_samples = X_train.shape[0] \n",
        "n_classes = 10 \n",
        "\n",
        "#計算グラフに渡す引数の形を決める\n",
        "X = tf.placeholder('float', [None, n_input])\n",
        "Y = tf.placeholder('float', [None, n_classes])\n",
        "\n",
        "#trainのミニバッチイテレータ\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "\n",
        "def example_net(x):\n",
        "    '''\n",
        "    単純な3層ニューラルネットワーク\n",
        "    '''\n",
        "    #重みとバイアスの宣言\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    \n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "    \n",
        "    #全結合層\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] #addと同じ\n",
        "    \n",
        "    return layer_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7z7QjdeqVM3H",
        "outputId": "aee3abc4-02bd-4bf1-d6aa-389f2c068574"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ネットワーク構造の読み込み\n",
        "logits = example_net(X)\n",
        "\n",
        "#目的関数\n",
        "#目的関数\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
        "\n",
        "#最適化手法\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "#推定結果\n",
        "correct_pred = tf.equal(tf.argmax(Y, axis=1), tf.argmax(tf.nn.softmax(logits), axis=1))\n",
        "\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# \n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # ミニバッチごとにループ\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "            total_acc += acc\n",
        "\n",
        "        total_loss /= total_batch\n",
        "        total_acc /= total_batch\n",
        "        val_loss= sess.run(loss_op, feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n",
        "    \n",
        "    #estimation   \n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
        "    print('test_acc : {:.3f}'.format(test_acc))"
      ],
      "metadata": {
        "id": "LFN9XmSWVhZE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8867bf1-a276-4e1e-da9c-885b4b3f9674"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 5.2121, val_loss : 1.7513, acc : 0.460\n",
            "Epoch 1, loss : 1.4545, val_loss : 1.2738, acc : 0.630\n",
            "Epoch 2, loss : 1.0644, val_loss : 1.0179, acc : 0.670\n",
            "Epoch 3, loss : 0.8634, val_loss : 0.8603, acc : 0.720\n",
            "Epoch 4, loss : 0.7368, val_loss : 0.7177, acc : 0.750\n",
            "Epoch 5, loss : 0.6206, val_loss : 0.5989, acc : 0.770\n",
            "Epoch 6, loss : 0.5194, val_loss : 0.5303, acc : 0.860\n",
            "Epoch 7, loss : 0.4429, val_loss : 0.4761, acc : 0.920\n",
            "Epoch 8, loss : 0.3647, val_loss : 0.3705, acc : 0.980\n",
            "Epoch 9, loss : 0.2823, val_loss : 0.3151, acc : 0.970\n",
            "test_acc : 0.923\n"
          ]
        }
      ]
    }
  ]
}