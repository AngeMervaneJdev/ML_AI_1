{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SimplConv1d.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOYu7E+ttkAbqEezCUhOXcd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AngeMervaneJdev/ML_AI_1/blob/main/SimplConv1d.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#[Problème 1] Création d'une classe de couche convolutionnelle unidimensionnelle qui limite le nombre de canaux à un"
      ],
      "metadata": {
        "id": "idVsICkXCSpU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np;\n",
        "from scipy.sparse import dia_matrix\n",
        "from keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ],
      "metadata": {
        "id": "iJFPqImGcac1"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "fkXtlSabCJi7"
      },
      "outputs": [],
      "source": [
        "\n",
        "class SimpleConv1d:\n",
        "    def __init__(self, weight, baias):\n",
        "        self.optimizer = None\n",
        "\n",
        "        self.W = weight\n",
        "        self.B = baias\n",
        "        self.dW = None\n",
        "        self.dB = None\n",
        "        self.idx =None\n",
        "        self.X = None\n",
        "        pass\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.a = X.copy()\n",
        "        idx1 = np.arange(self.W.shape[0])\n",
        "        idx2 = np.arange(self.W.shape[0]-1 ).reshape(-1, 1)\n",
        "        self.idx = idx1 + idx2\n",
        "        A = np.sum(X[self.idx] * self.W.T,axis=1) + self.B\n",
        "        return A\n",
        "    def backward(self, dX,dA):\n",
        "\n",
        "        dB = np.sum(dA,axis=0)\n",
        "        dW = np.sum(dA[:,np.newaxis] *dX[self.idx],axis=0)\n",
        "        da = dA.reshape(-1,1)\n",
        "        data= np.repeat(da,dX.shape[0],axis=1)\n",
        "        offsets= np.arange(self.output(data))\n",
        "        d = dia_matrix((data,offsets),shape=(w.shape[0],x.shape[0])).toarray()\n",
        "        dx = np.sum(d * w[:,np.newaxis],axis=0)\n",
        "        \n",
        "        return dX\n",
        "    def output(self,X):\n",
        "        n_in = X.shape[1]\n",
        "        p = 0\n",
        "        f = self.W.shape[0]\n",
        "        s = 1 \n",
        "        n_out = ((n_in + 2*p - f)/s)+1\n",
        "        return n_out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#[Problème 2] Calcul de la taille de sortie après convolution unidimensionnelle"
      ],
      "metadata": {
        "id": "dVAuGo-NEfTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def output(X,p,f,s):\n",
        "    n_in = X.shape[0]\n",
        "    n_out = ((n_in + 2*p - f)/s)+1\n",
        "    return n_out"
      ],
      "metadata": {
        "id": "tGh6cqtlEYOu"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#[Problème 3] Expérience d'une couche convolutionnelle unidimensionnelle avec un petit réseau"
      ],
      "metadata": {
        "id": "kSgVREq-EsLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([1,2,3,4])\n",
        "w = np.array([3, 5, 7])\n",
        "b = np.array([1])\n",
        "x.shape,w.shape,b.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wj3-M5PDcROz",
        "outputId": "db35e22a-143b-4785-db36-5c6a81c01598"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4,), (3,), (1,))"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc1 = SimpleConv1d(w,b)\n",
        "sc1.forward(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vc0SCtQUc5ew",
        "outputId": "86e01be8-a739-40e1-8f3b-1df6b62d51b1"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([35, 50])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "delta_a = np.array([10, 20])\n",
        "delta_a.shape\n",
        "delta_b = np.array([30])\n",
        "delta_w = np.array([50, 80, 110])\n",
        "delta_x = np.array([30, 110, 170, 140])"
      ],
      "metadata": {
        "id": "hfdpQwKJcZRv"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc1 = SimpleConv1d(delta_w,delta_b)\n",
        "sc1.backward(delta_x,delta_a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GN8VZZDch3GZ",
        "outputId": "3e28f0e5-a62a-4fab-f6a8-0f038ab9ac24"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 30, 110, 170, 140])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([1, 2, 3, 4])\n",
        "w = np.array([3, 5, 7])\n",
        "\n",
        "a = np.empty((2, 3))\n",
        "\n",
        "indexes0 = np.array([0, 1, 2])\n",
        "indexes1 = np.array([1, 2, 3])\n",
        "\n",
        "a[0] = x[indexes0]*w # x[indexes0]は([1, 2, 3])である\n",
        "a[1] = x[indexes1]*w # x[indexes1]は([2, 3, 4])である\n",
        "\n",
        "a = a.sum(axis=1)\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdunwE2AnOJj",
        "outputId": "f8cb695d-643c-4030-a5d2-dcfd006dbb0c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([34., 49.])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#[Problème 4] Création d'une classe de couche convolutionnelle unidimensionnelle qui ne limite pas le nombre de canaux"
      ],
      "metadata": {
        "id": "tfe5OwOtj_CX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]]) # \n",
        "w = np.ones((3, 2, 3)) # \n",
        "b = np.array([1, 2, 3])"
      ],
      "metadata": {
        "id": "J6ZimcBOj-oG"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def output2(X,p,f,s):\n",
        "    n_in = X.shape[1]\n",
        "    p = p #\n",
        "    f = f #\n",
        "    s = s #\n",
        "    n_out = ((n_in + 2*p - f)/s)+1\n",
        "    return n_out"
      ],
      "metadata": {
        "id": "KR29GVrb60bI"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.array([[16, 22], [17, 23], [18, 24]])\n",
        "o = int(output2(x,0,w.shape[-1],1))\n",
        "print(o)\n",
        "\n",
        "nin, i = x.shape\n",
        "print(nin,i)\n",
        "oc, c, f = w.shape\n",
        "print(oc,c,f)\n",
        "\n",
        "#c * f \n",
        "idx_arr = np.arange(c*f).reshape(c,f)\n",
        "idx_arr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVA3zh6jwDwu",
        "outputId": "906d3ff6-0eec-4f32-b36e-c68a2b39ea93"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "2 4\n",
            "3 2 3\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1, 2],\n",
              "       [3, 4, 5]])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx1 = np.arange(w.shape[-1])\n",
        "\n",
        "idx2 = np.arange(w.shape[-1]-1 ).reshape(-1, 1)\n",
        "\n",
        "idx = idx1 + idx2\n",
        "\n",
        "print(x[:,idx].shape)\n",
        "a1 = x[:,idx]\n",
        "a2 = w\n",
        "print(a1)\n",
        "a= np.zeros((f,o))\n",
        "for i in range(f):\n",
        "    for j in range(o):\n",
        "        x_in = a1[j]\n",
        "        print(x_in.shape)\n",
        "        a[i][j] = np.sum(x_in @ w[i][j])\n",
        "        print('a',a.shape)\n",
        "    print(b[i].shape)\n",
        "    a[i] += a[i] + b[i]\n",
        "    print(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZH_DUoF7OjL",
        "outputId": "5c1e8de0-5169-4657-bf08-4ae586330c23"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 2, 3)\n",
            "[[[1 2 3]\n",
            "  [2 3 4]]\n",
            "\n",
            " [[2 3 4]\n",
            "  [3 4 5]]]\n",
            "(2, 3)\n",
            "a (3, 2)\n",
            "(2, 3)\n",
            "a (3, 2)\n",
            "()\n",
            "[[31. 43.]\n",
            " [ 0.  0.]\n",
            " [ 0.  0.]]\n",
            "(2, 3)\n",
            "a (3, 2)\n",
            "(2, 3)\n",
            "a (3, 2)\n",
            "()\n",
            "[[31. 43.]\n",
            " [32. 44.]\n",
            " [ 0.  0.]]\n",
            "(2, 3)\n",
            "a (3, 2)\n",
            "(2, 3)\n",
            "a (3, 2)\n",
            "()\n",
            "[[31. 43.]\n",
            " [32. 44.]\n",
            " [33. 45.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_output_size(n_in, P, F, S):\n",
        "    return int(1 + (n_in + 2*P - F) / S)"
      ],
      "metadata": {
        "id": "e0J-pR55zuEt"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FC:\n",
        "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
        "        self.B = initializer.B(n_nodes2)\n",
        "\n",
        "    \n",
        "    def forward(self, Z1):\n",
        "        self.Z = Z1.copy()\n",
        "        Z2 = np.dot(Z1, self.W) + self.B\n",
        "        \n",
        "        return Z2\n",
        "    \n",
        "    \n",
        "    def backward(self, dA):\n",
        "        self.dB = dA \n",
        "        self.dW = np.dot(self.Z.T, dA) \n",
        "        dZ = np.dot(dA, self.W.T) \n",
        "        \n",
        "        self = self.optimizer.update(self)\n",
        "        \n",
        "        return dZ"
      ],
      "metadata": {
        "id": "HOD2wMfZjx2E"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GetMiniBatch:\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]        \n",
        "\n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]"
      ],
      "metadata": {
        "id": "NkZK8LeiQWT8"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Flat():\n",
        "    def __init__(self):\n",
        "        self.X_shape = None\n",
        "    \n",
        "    def forward(self, X):\n",
        "        X_1d = X.reshape(X.shape[0], -1)\n",
        "        \n",
        "        self.X_shape = X.shape\n",
        "        \n",
        "        return X_1d\n",
        "\n",
        "    def backward(self, X):\n",
        "        X = X.reshape(self.X_shape)\n",
        "        \n",
        "        return X"
      ],
      "metadata": {
        "id": "m-tMBy6fR8vz"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Tanh:\n",
        "    def __init__(self):\n",
        "        self.Z = None\n",
        "    \n",
        "    def forward(self, A):\n",
        "        self.Z =  np.tanh(A)\n",
        "        \n",
        "        return  self.Z\n",
        "    \n",
        "    def backward(self, dZ):\n",
        "        dA = dZ * (1 - self.Z**2)\n",
        "        \n",
        "        return dA"
      ],
      "metadata": {
        "id": "XcKPVd6sVvCu"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class XavierInitializer:\n",
        "    def __init__(self):\n",
        "        pass    \n",
        "    def W(self, n_nodes1=None, n_nodes2=None, \n",
        "                  filter_num=None, C=None, filter_size=None):\n",
        "        #\n",
        "        if filter_num and C and filter_size is not None:\n",
        "            W =  np.random.randn(filter_num, C, filter_size) / np.sqrt(filter_num)\n",
        "        #\n",
        "        else:\n",
        "            W = np.random.randn(n_nodes1, n_nodes2) / np.sqrt(n_nodes1) \n",
        "        return W\n",
        "    \n",
        "    def B(self, n_nodes2):\n",
        "        B = np.zeros(n_nodes2) \n",
        "        \n",
        "        return B"
      ],
      "metadata": {
        "id": "uCQegX7EVrbJ"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        self.Z = None \n",
        "    def forward(self, A):\n",
        "        self.Z = 1 / (1 + np.exp(-A))\n",
        "        return self.Z\n",
        "    def backward(self, dZ):\n",
        "        dA = dZ * (1 - self.Z) * self.Z\n",
        "        return dA"
      ],
      "metadata": {
        "id": "MJLMi0tnVztM"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HeInitializer:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def W(self, n_nodes1=None, n_nodes2=None,filter_num=None, C=None, filter_size=None):\n",
        "      \n",
        "        if filter_num and C and filter_size is not None:\n",
        "            W = np.random.randn(filter_num, C, filter_size) * np.sqrt(2 / filter_num)\n",
        "        else:\n",
        "            W = np.random.randn(n_nodes1, n_nodes2) * np.sqrt(2 / n_nodes1)\n",
        "        return W\n",
        "\n",
        "    def B(self, n_nodes2):\n",
        "        B = np.zeros(n_nodes2) \n",
        "        return B"
      ],
      "metadata": {
        "id": "EMHOnCrEVnXo"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdaGrad:\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr \n",
        "        self.H_W= None \n",
        "        self.H_B = None\n",
        "\n",
        "    def update(self, layer):\n",
        "        if self.H_W is None:\n",
        "            self.H_W = np.zeros(layer.W.shape)\n",
        "        if self.H_B is None:\n",
        "            self.H_B = np.zeros(layer.B.shape)\n",
        "        \n",
        "        self.H_W += (layer.dW / layer.dB.shape[0]) ** 2 \n",
        "        self.H_B += (layer.dB.mean(axis=0)) **2\n",
        "        layer.W -= self.alpha / np.sqrt(self.H_W + 1e-7) * layer.dW / layer.dB.shape[0] \n",
        "        layer.B -= self.alpha / np.sqrt(self.H_B + 1e-7) * layer.dB.mean(axis=0) \n",
        "        \n",
        "        return layer"
      ],
      "metadata": {
        "id": "iaya9vvoVhO5"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReLU:\n",
        "    def __init__(self):\n",
        "        self.X = None\n",
        "    def forward(self, A):\n",
        "        self.X = A.copy()\n",
        "        Z = np.maximum(0, A)\n",
        "        return Z\n",
        "    def backward(self, dZ):\n",
        "        dA = np.where(self.X > 0, dZ, 0)\n",
        "        return dA"
      ],
      "metadata": {
        "id": "UhM8XSu9WSOM"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SGD:\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "    def update(self, layer):\n",
        "        ## layer.dB=partial derivatives of loss\n",
        "        layer.W -= self.lr* layer.dW / layer.dB.shape[0] #(n_nodes1, n_nodes2)\n",
        "        layer.B -= self.lr* layer.dB.mean(axis=0) #(n_nodes2)"
      ],
      "metadata": {
        "id": "TtMbyj2XWNF8"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Softmax:\n",
        "    def __init__(self):\n",
        "        self.Z = None\n",
        "    \n",
        "    def forward(self, A):\n",
        "        c = np.max(A)\n",
        "        exp_A = np.exp(A - c)\n",
        "        sum_exp_A = np.sum(exp_A, axis=1).reshape(-1, 1)\n",
        "\n",
        "        self.Z = exp_A / sum_exp_A  \n",
        "        return self.Z\n",
        "\n",
        "    def backward(self, y):\n",
        "        loss_sum = np.sum(y * np.log(self.Z), axis=1)\n",
        "        loss = -np.mean(loss_sum)\n",
        "        \n",
        "        dA = self.Z - y    \n",
        "        return dA"
      ],
      "metadata": {
        "id": "G_8vyYLvV6mP"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv1d:\n",
        "    \"\"\"\n",
        "    A class of a convolution 1D layer\n",
        "    Parameters\n",
        "    ----------\n",
        "    F : int\n",
        "      filter size\n",
        "    ch_in : int\n",
        "      input channels\n",
        "    ch_out : int\n",
        "      output channels\n",
        "    initializer: instance of initialization method\n",
        "    optimizer: instance of optimization method\n",
        "    \"\"\"\n",
        "    def __init__(self, F, ch_in, ch_out, initializer, optimizer):\n",
        "        self.F = F\n",
        "        self.ch_in = ch_in\n",
        "        self.ch_out = ch_out\n",
        "        self.optimizer = optimizer\n",
        "        self.W = np.empty((ch_out, ch_in, F))\n",
        "        for ch in np.arange(0, ch_out):\n",
        "            self.W[ch] = initializer.W(ch_in, F)\n",
        "        self.B = initializer.B(ch_out)\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        forward propagation, finding A\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : (ch_in, number of features)\n",
        "        Return\n",
        "        ----------\n",
        "        A : (number of output channels, n_out)\n",
        "        \"\"\"\n",
        "        self.X = X\n",
        "        self.n_out = compute_output_size(self.X.shape[1], 0, self.F, 1)\n",
        "        A = np.empty((self.ch_out, self.n_out))\n",
        "        for ch in np.arange(0, self.ch_out):\n",
        "            for i in np.arange(0, self.n_out):\n",
        "                A[ch, i] =np.dot( np.sum(X[:, i:i+self.F] ,self.W[ch]) + self.B[ch])\n",
        "        return A\n",
        "\n",
        "    def backward(self, dA):\n",
        "        \"\"\"\n",
        "        Backward\n",
        "        Parameters\n",
        "        ----------\n",
        "        dA : The following forms of ndarray, shape (ch_out, n_out)\n",
        "            Gradient flowing from behind\n",
        "        Returns\n",
        "        ----------\n",
        "        dZ : The following forms of ndarray, shape (ch_in, number of features)\n",
        "            Gradient to flow forward\n",
        "        \"\"\"\n",
        "        #update dW\n",
        "        self.dW = np.empty((self.ch_out, self.ch_in, self.F))\n",
        "        for c_out in np.arange(0, self.ch_out):\n",
        "            for c_in in np.arange(0, self.ch_in):\n",
        "                for s in np.arange(0, self.F):\n",
        "                    self.dW[c_out, c_in, s] = np.sum(dA[c_out] * self.X[c_in, s:s+self.n_out])\n",
        "        #update dB\n",
        "        self.dB = np.sum(dA, axis=1)\n",
        "        #update dZ\n",
        "        self.dZ = np.empty((self.ch_in, self.X.shape[1]))\n",
        "        for c_in in np.arange(0, self.ch_in):\n",
        "            for j in np.arange(0, self.X.shape[1]):\n",
        "                total = 0.0\n",
        "                for c_out in np.arange(0, self.ch_out):\n",
        "                    for s in np.arange(0, self.F):\n",
        "                        if (j - s >= 0 and j - s < self.n_out):\n",
        "                            total += dA[c_out, j - s] * self.W[c_out, c_in, s]\n",
        "                self.dZ[c_in, j] = total\n",
        "        # update new W, B\n",
        "        self = self.optimizer.update(self)\n",
        "        return self.dZ"
      ],
      "metadata": {
        "id": "Dz4-cTwS-PYa"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleInitializer:\n",
        "    def __init__(self, sigma):\n",
        "        self.sigma = sigma \n",
        "        \n",
        "\n",
        "    def W(self, n_nodes1=None, n_nodes2=None,filter_num=None, C=None, filter_size=None):\n",
        "\n",
        "        if filter_num is not None and C is not None and filter_size is not None:\n",
        "            W =  self.sigma * np.random.randn(filter_num, C, filter_size) \n",
        "        if n_nodes1 is not None and n_nodes2 is not None:\n",
        "            W =  self.sigma * np.random.randn(n_nodes1, n_nodes2) \n",
        "        return W\n",
        "    \n",
        "    \n",
        "    def B(self, n_nodes2):\n",
        "        B = np.zeros(n_nodes2) \n",
        "        return B"
      ],
      "metadata": {
        "id": "3KD9hYb9b8YR"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Scratchch1dCNNNeuralNetrowkClassifier():\n",
        "    def __init__(self, epoc=10, activation='relu', solver='adagrad', alpha=0.005, batch_size=10, initial='he', sigma=0.01, n_nodes1=50,n_nodes2=25, filter_num=3,  filter_size=7, verbose=True):\n",
        "        self.epoc= epoc  \n",
        "        self.activation=activation \n",
        "        self.solver=solver \n",
        "        self.alpha=alpha\n",
        "        self.batch_size=batch_size\n",
        "        self.initial=initial    \n",
        "        self.sigma=sigma         \n",
        "        self.n_nodes1=n_nodes1  \n",
        "        self.n_nodes2=n_nodes2   \n",
        "        self.filter_num=filter_num   \n",
        "        self.filter_size=filter_size    \n",
        "        self.verbose=verbose        \n",
        "        \n",
        "        \n",
        "        self.conv1=None\n",
        "        self.FC1= None \n",
        "        self.FC2= None\n",
        "        self.activation1= None \n",
        "        self.activation2= None\n",
        "        self.activation3= None \n",
        "        self.loss_list= None \n",
        "        self.mini_loss_list= None \n",
        "        self.val_loss_list= None\n",
        "        self.mini_loss_list= None \n",
        "        self.flat=None\n",
        "    \n",
        "    \n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "\n",
        "        #one_hot\n",
        "        n_output = np.unique(y).shape[0]\n",
        "        y_onehot = self._one_hot(y, n_output)\n",
        "        \n",
        "        #\n",
        "        train_mini_batch = GetMiniBatch(X, y_onehot, self.batch_size)\n",
        "        \n",
        "        #val\n",
        "        if X_val is not None and y_val is not None:       \n",
        "            y_val_onehot = self._one_hot(y_val, n_output) #one-hot\n",
        "            test_mini_batch = GetMiniBatch(X_val, y_val_onehot) #\n",
        "        \n",
        "        \n",
        "        #\n",
        "        if self.activation == 'sigmoid':\n",
        "            activate1 = Sigmoid()\n",
        "            activate2 = Sigmoid()\n",
        "            activate3 = Sigmoid()\n",
        "        elif self.activation == 'tanh':\n",
        "            activate1 = Tanh()\n",
        "            activate2 = Tanh()\n",
        "            activate3 = Tanh()\n",
        "        elif self.activation == 'relu':\n",
        "            activate1 = ReLU()\n",
        "            activate2 = ReLU()\n",
        "            activate3 = ReLU()\n",
        "\n",
        "        \n",
        "        #\n",
        "        if self.solver == 'sgd':\n",
        "            optimizer1 = SGD(self.alpha)\n",
        "            optimizer2 = SGD(self.alpha)\n",
        "            optimizer3 = SGD(self.alpha) \n",
        "            optimizer4 = SGD(self.alpha)\n",
        "            optimizer5 = SGD(self.alpha) \n",
        "        elif self.solver == 'adagrad':\n",
        "            optimizer1 = AdaGrad(self.alpha)\n",
        "            optimizer2 = AdaGrad(self.alpha)\n",
        "            optimizer3 = AdaGrad(self.alpha)\n",
        "            optimizer4 = AdaGrad(self.alpha)\n",
        "            optimizer5 = AdaGrad(self.alpha)\n",
        "            \n",
        "        #\n",
        "        if self.initial == 'simple':\n",
        "            initializer1 = SimpleInitializer(self.sigma)\n",
        "            initializer2 = SimpleInitializer(self.sigma)\n",
        "            initializer3 = SimpleInitializer(self.sigma)\n",
        "            initializer4 = SimpleInitializer(self.sigma)\n",
        "            initializer5 = SimpleInitializer(self.sigma)\n",
        "        elif self.initial == 'xavier':\n",
        "            initializer1 = XavierInitializer()\n",
        "            initializer2 = XavierInitializer()\n",
        "            initializer3 = XavierInitializer()\n",
        "            initializer4 = XavierInitializer()\n",
        "            initializer5 = XavierInitializer()\n",
        "        elif self.initial == 'he':\n",
        "            initializer1 = HeInitializer()\n",
        "            initializer2 = HeInitializer()\n",
        "            initializer3 = HeInitializer()\n",
        "            initializer4 = HeInitializer()\n",
        "            initializer5 = HeInitializer()\n",
        "\n",
        "        #\n",
        "       # self.conv1 = Conv1d(initializer1, optimizer1, self.filter_num, X.shape[1], self.filter_size)\n",
        "        self.conv1 = Conv1d(self.filter_num, X.shape[1], self.filter_size,initializer1, optimizer1)\n",
        "\n",
        "        self.activation1 = activate1\n",
        "        \n",
        "        #\n",
        "        self.flat = Flat()\n",
        "        out = self.filter_num * (X.shape[1] - (self.filter_size - 1))\n",
        "\n",
        "        #\n",
        "        self.FC1 = FC(out, self.n_nodes1, initializer2, optimizer2) \n",
        "        self.activation2 = activate2\n",
        "        self.FC2 = FC(self.n_nodes1, self.n_nodes2, initializer3, optimizer3) \n",
        "        self.activation3 = activate3\n",
        "        self.FC3 = FC(self.n_nodes2, y_onehot.shape[1], initializer4, optimizer4)\n",
        "        self.activation4 = Softmax()\n",
        "        \n",
        "        #\n",
        "        self.loss_list=[]\n",
        "        self.val_loss_list=[]\n",
        "        \n",
        "        #\n",
        "        for i in range(self.epoc):\n",
        "            \n",
        "            #\n",
        "            self.mini_loss_list = []\n",
        "            \n",
        "            #\n",
        "            for mini_X_train, mini_y_train in train_mini_batch:\n",
        "                \n",
        "                #\n",
        "                A1 = self.conv1.forward(mini_X_train) \n",
        "                Z1 = self.activation1.forward(A1)    \n",
        "                F1 = self.flat.forward(Z1)\n",
        "                \n",
        "                #\n",
        "                A2 = self.FC1.forward(F1)                   \n",
        "                Z2 = self.activation2.forward(A2) \n",
        "                A3 = self.FC2.forward(Z2)             \n",
        "                Z3 = self.activation3.forward(A3)\n",
        "                A4 = self.FC3.forward(Z3)             \n",
        "                Z4 = self.activation4.forward(A4)   \n",
        "                \n",
        "                #\n",
        "                dA4, mini_loss = self.activation4.backward(mini_y_train) \n",
        "                dZ4 = self.FC3.backward(dA4)          \n",
        "                dA3 = self.activation3.backward(dZ4) \n",
        "                dZ3 = self.FC2.backward(dA3)            \n",
        "                dA2 = self.activation2.backward(dZ3) \n",
        "                dZ2 = self.FC1.backward(dA2)    \n",
        "                \n",
        "                #\n",
        "                dF1 = self.flat.backward(dZ2)\n",
        "                \n",
        "                \n",
        "                dA2 = self.activation1.backward(dF1)\n",
        "                dZ1 = self.conv1.backward(dA2)\n",
        "\n",
        "                #\n",
        "                self.mini_loss_list.append(mini_loss)\n",
        "\n",
        "            #\n",
        "            loss = np.mean(self.mini_loss_list)\n",
        "            self.loss_list.append(loss)\n",
        "\n",
        "            \n",
        "            #\n",
        "            if X_val is not None and y_val is not None:\n",
        "                \n",
        "                self.mini_val_loss_list = []\n",
        "                for mini_X_val, mini_y_val in test_mini_batch:\n",
        "              \n",
        "                    #\n",
        "                    A1 = self.conv1.forward(mini_X_val)\n",
        "                    Z1 = self.activation1.forward(A1)\n",
        "                    F1 = self.flat.forward(Z1)\n",
        "                    A2 = self.FC1.forward(F1)\n",
        "                    Z2 = self.activation2.forward(A2)\n",
        "                    A3 = self.FC2.forward(Z2)\n",
        "                    Z3 = self.activation3.forward(A3)\n",
        "                    A4 = self.FC3.forward(Z3) \n",
        "                    Z4 = self.activation4.forward(A4)\n",
        "\n",
        "                    #\n",
        "                    _, mini_val_loss = self.activation4.backward(mini_y_val)\n",
        "                    self.mini_val_loss_list.append(mini_val_loss)\n",
        "\n",
        "                val_loss = np.mean(self.mini_val_loss_list)\n",
        "                self.val_loss_list.append(val_loss)\n",
        "\n",
        "                \n",
        "            #\n",
        "            if self.verbose == True:\n",
        "                print('Processus d\\'apprentissage des données de vérification' + str(i + 1) + 'epoc : ' + str(self.loss_list[i]))\n",
        "                #\n",
        "            if X_val is not None or y_val is not None:\n",
        "                print('Processus d\\'apprentissage des données de vérification' + str(i + 1) + 'epoc: ' + str(self.val_loss_list[i]))\n",
        "                    \n",
        "                \n",
        "    def predict(self, X):\n",
        "    \n",
        "        #\n",
        "        A1 = self.conv1.forward(X) \n",
        "        Z1 = self.activation1.forward(A1) \n",
        "        \n",
        "        #\n",
        "        F1 = self.flat.forward(Z1)\n",
        "        \n",
        "        #\n",
        "        A2 = self.FC1.forward(F1)\n",
        "        Z2 = self.activation2.forward(A2)\n",
        "        A3 = self.FC2.forward(Z2)\n",
        "        Z3 = self.activation3.forward(A3)\n",
        "        A4 = self.FC3.forward(Z3)\n",
        "        y_pred = self.activation4.forward(A4)  \n",
        "                       \n",
        "        return np.argmax(y_pred, axis=1)\n",
        "            \n",
        "\n",
        "    def _one_hot(self, y, n_output):\n",
        "        #\n",
        "        one_hot = np.zeros((n_output, y.shape[0]))\n",
        "        \n",
        "        #\n",
        "        for idx, val in enumerate(y.astype(int)):\n",
        "            one_hot[val, idx] = 1\n",
        "\n",
        "        return one_hot.T"
      ],
      "metadata": {
        "id": "r2gLP06hXKsp"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train.reshape(-1, 784)\n",
        "X_test = X_test.reshape(-1, 784)\n",
        "\n",
        "\n",
        "X_train = X_train.astype(np.float)\n",
        "X_test = X_test.astype(np.float)\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "print(X_train.max()) # 1.0\n",
        "print(X_train.min()) # 0.0\n",
        "\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
        "print(X_train.shape) # (48000, 784)\n",
        "print(X_val.shape) # (12000, 784)"
      ],
      "metadata": {
        "id": "D8rrgIP0bjET",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f00d5c02-9596-45a6-b166-1ef892f7583a"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  import sys\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n",
            "0.0\n",
            "(48000, 784)\n",
            "(12000, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Scratchch1dCNNNeuralNetrowkClassifier()\n",
        "\n",
        "model.fit(X_train,y_train,X_val,y_val)"
      ],
      "metadata": {
        "id": "zDiE8Ygcbl5n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}